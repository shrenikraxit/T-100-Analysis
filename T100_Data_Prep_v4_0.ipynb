{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dcRn2GIPiEN5"
      ],
      "authorship_tag": "ABX9TyOizEjUhb+TMqKJoyCuP5sf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrenikraxit/T-100-Analysis/blob/main/T100_Data_Prep_v4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Concatenate the T-100 files\n",
        "\n",
        "Year = 2018, 2019, 2023 and 2024"
      ],
      "metadata": {
        "id": "WGgCuPeafd8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oq3E1El666e",
        "outputId": "ef3d7f44-221b-4656-b146-68af45bde933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 0 - Environment preparation\n",
        "\n",
        "# Connect with google drive (for Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "def consolidate_t100_simple(data_directory, years=None):\n",
        "\n",
        "    print(f\"üîç Looking for T-100 files in: {data_directory}\")\n",
        "\n",
        "    # Find all T-100 files\n",
        "    pattern = str(Path(data_directory) / \"T_T100_MARKET_ALL_CARRIER *.csv\")\n",
        "    files = sorted(glob.glob(pattern))\n",
        "\n",
        "    if not files:\n",
        "        print(f\"‚ùå No files found matching: T_T100_MARKET_ALL_CARRIER *.csv\")\n",
        "        print(f\"Files in directory: {list(Path(data_directory).glob('*.csv'))}\")\n",
        "        return None\n",
        "\n",
        "    # Filter by years if specified\n",
        "    if years:\n",
        "        files = [f for f in files if any(str(year) in Path(f).name for year in years)]\n",
        "\n",
        "    print(f\"üìÅ Found {len(files)} files:\")\n",
        "    for file in files:\n",
        "        size_mb = Path(file).stat().st_size / (1024**2)\n",
        "        print(f\"   ‚Ä¢ {Path(file).name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    # Read and combine all files\n",
        "    print(f\"\\nüìä Reading and combining files...\")\n",
        "\n",
        "    all_dataframes = []\n",
        "    total_rows = 0\n",
        "\n",
        "    for file in files:\n",
        "        print(f\"   Reading {Path(file).name}...\", end=\" \")\n",
        "\n",
        "        # Read file in chunks to handle large files\n",
        "        chunks = []\n",
        "        for chunk in pd.read_csv(file, chunksize=50000):\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        df = pd.concat(chunks, ignore_index=True)\n",
        "        all_dataframes.append(df)\n",
        "\n",
        "        rows = len(df)\n",
        "        total_rows += rows\n",
        "        print(f\"{rows:,} rows\")\n",
        "\n",
        "    # Combine all dataframes\n",
        "    print(f\"\\nüîó Combining {len(all_dataframes)} files...\")\n",
        "    combined_data = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "    # Basic info\n",
        "    print(f\"‚úÖ Combined dataset:\")\n",
        "    print(f\"   ‚Ä¢ Total rows: {len(combined_data):,}\")\n",
        "    print(f\"   ‚Ä¢ Total columns: {len(combined_data.columns)}\")\n",
        "    if 'YEAR' in combined_data.columns:\n",
        "        years_found = sorted(combined_data['YEAR'].unique())\n",
        "        print(f\"   ‚Ä¢ Years: {years_found}\")\n",
        "    if 'PASSENGERS' in combined_data.columns:\n",
        "        total_pax = combined_data['PASSENGERS'].sum()\n",
        "        print(f\"   ‚Ä¢ Total passengers: {total_pax:,.0f}\")\n",
        "\n",
        "    # Save consolidated file\n",
        "    output_file = Path(data_directory) / \"consolidated_t100_data.csv\"\n",
        "    print(f\"\\nüíæ Saving to: {output_file}\")\n",
        "\n",
        "    combined_data.to_csv(output_file, index=False)\n",
        "\n",
        "    size_mb = output_file.stat().st_size / (1024**2)\n",
        "    print(f\"‚úÖ Saved! File size: {size_mb:.1f} MB\")\n",
        "\n",
        "    return str(output_file)\n",
        "\n",
        "# ===== USAGE =====\n",
        "\n",
        "# UPDATE THIS PATH:\n",
        "DATA_DIRECTORY = \"/content/drive/MyDrive/airline_data_analysis_v2/\"  # <-- CHANGE THIS!\n",
        "\n",
        "# OPTION 1: Consolidate all years\n",
        "consolidated_file = consolidate_t100_simple(DATA_DIRECTORY, years=[2018, 2019, 2023, 2024])\n",
        "\n",
        "# Print result\n",
        "if consolidated_file:\n",
        "    print(f\"\\nüéâ SUCCESS!\")\n",
        "    print(f\"   Consolidated file: {consolidated_file}\")\n",
        "    print(f\"\\nüìã Next step - use in your seasonal analysis:\")\n",
        "    print(f'   filepath = \"{consolidated_file}\"')\n",
        "    print(f'   results = run_seasonal_analysis(filepath, years=[2018, 2019, 2023, 2024]')\n",
        "else:\n",
        "    print(f\"\\n‚ùå Failed. Please check your directory path.\")\n",
        "\n",
        "# If you just want to test with one year:\n",
        "# test_file = consolidate_t100_simple(DATA_DIRECTORY, years=[2019])  # Just 2019 for testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ih5a5fj7W87",
        "outputId": "ec3a7248-3a17-49ad-9aab-4489fbe4b44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Looking for T-100 files in: /content/drive/MyDrive/airline_data_analysis_v2/\n",
            "üìÅ Found 4 files:\n",
            "   ‚Ä¢ T_T100_MARKET_ALL_CARRIER 2018.csv (80.8 MB)\n",
            "   ‚Ä¢ T_T100_MARKET_ALL_CARRIER 2019.csv (80.6 MB)\n",
            "   ‚Ä¢ T_T100_MARKET_ALL_CARRIER 2023.csv (82.6 MB)\n",
            "   ‚Ä¢ T_T100_MARKET_ALL_CARRIER 2024.csv (80.6 MB)\n",
            "\n",
            "üìä Reading and combining files...\n",
            "   Reading T_T100_MARKET_ALL_CARRIER 2018.csv... 321,478 rows\n",
            "   Reading T_T100_MARKET_ALL_CARRIER 2019.csv... 321,388 rows\n",
            "   Reading T_T100_MARKET_ALL_CARRIER 2023.csv... 327,440 rows\n",
            "   Reading T_T100_MARKET_ALL_CARRIER 2024.csv... 320,910 rows\n",
            "\n",
            "üîó Combining 4 files...\n",
            "‚úÖ Combined dataset:\n",
            "   ‚Ä¢ Total rows: 1,291,216\n",
            "   ‚Ä¢ Total columns: 41\n",
            "   ‚Ä¢ Years: [np.int64(2018), np.int64(2019), np.int64(2023), np.int64(2024)]\n",
            "   ‚Ä¢ Total passengers: 4,242,158,055\n",
            "\n",
            "üíæ Saving to: /content/drive/MyDrive/airline_data_analysis_v2/consolidated_t100_data.csv\n",
            "‚úÖ Saved! File size: 327.2 MB\n",
            "\n",
            "üéâ SUCCESS!\n",
            "   Consolidated file: /content/drive/MyDrive/airline_data_analysis_v2/consolidated_t100_data.csv\n",
            "\n",
            "üìã Next step - use in your seasonal analysis:\n",
            "   filepath = \"/content/drive/MyDrive/airline_data_analysis_v2/consolidated_t100_data.csv\"\n",
            "   results = run_seasonal_analysis(filepath, years=[2018, 2019, 2023, 2024]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2\n",
        "T-100 Data Cleanup Script\n",
        "Comprehensive data quality checks and cleanup for consolidated_t100_data.csv\n",
        "\n",
        "This script performs the following cleanup operations:\n",
        "1. Remove cargo-only flights (PASSENGERS = 0)\n",
        "2. Check for missing/anomalous passenger counts\n",
        "3. Remove duplicate route entries\n",
        "4. Filter for domestic U.S. routes only\n",
        "5. Handle seasonal variations (quarterly aggregation)"
      ],
      "metadata": {
        "id": "9QF8EfM7UIje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "T-100 Data Cleanup Script\n",
        "Comprehensive data quality checks and cleanup for consolidated_t100_data.csv\n",
        "\n",
        "This script performs the following cleanup operations:\n",
        "1. Remove cargo-only flights (PASSENGERS = 0)\n",
        "2. Check for missing/anomalous passenger counts\n",
        "3. Filter for domestic U.S. routes only\n",
        "4. Remove duplicate route entries\n",
        "5. Handle seasonal variations (quarterly aggregation)\n",
        "\n",
        "NOTE: Airport-code validation has been removed per request.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up data directory path\n",
        "DATA_DIRECTORY = \"/content/drive/MyDrive/airline_data_analysis_v2/\"\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(DATA_DIRECTORY, exist_ok=True)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(f'{DATA_DIRECTORY}t100_cleanup.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "def setup_cleanup_summary():\n",
        "    \"\"\"Initialize cleanup summary tracking\"\"\"\n",
        "    return {\n",
        "        'original_rows': 0,\n",
        "        'cargo_only_removed': 0,\n",
        "        'missing_passengers_removed': 0,\n",
        "        'anomalous_passengers_removed': 0,\n",
        "        'non_domestic_removed': 0,\n",
        "        'duplicates_removed': 0,\n",
        "        'final_rows': 0,\n",
        "        'data_quality_issues': []\n",
        "    }\n",
        "\n",
        "def load_t100_data(filepath):\n",
        "    \"\"\"Load T-100 data with memory optimization\"\"\"\n",
        "    logging.info(f\"Loading T-100 data from {filepath}\")\n",
        "\n",
        "    try:\n",
        "        # Read in chunks to handle large files\n",
        "        chunk_list = []\n",
        "        chunk_size = 100000  # Adjust based on available memory\n",
        "\n",
        "        for chunk in pd.read_csv(filepath, chunksize=chunk_size, low_memory=False):\n",
        "            chunk_list.append(chunk)\n",
        "\n",
        "        df = pd.concat(chunk_list, ignore_index=True)\n",
        "\n",
        "        logging.info(f\"Successfully loaded {len(df):,} rows with {len(df.columns)} columns\")\n",
        "        logging.info(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading T-100 data: {e}\")\n",
        "        raise\n",
        "\n",
        "def cleanup_cargo_only_flights(df, summary):\n",
        "    \"\"\"Remove cargo-only flights (PASSENGERS = 0)\"\"\"\n",
        "    logging.info(\"Step 1: Removing cargo-only flights...\")\n",
        "\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Remove rows where PASSENGERS is 0 or NaN\n",
        "    cargo_mask = (df['PASSENGERS'] == 0) | (df['PASSENGERS'].isna())\n",
        "    cargo_count = cargo_mask.sum()\n",
        "\n",
        "    df_clean = df[~cargo_mask].copy()\n",
        "\n",
        "    summary['cargo_only_removed'] = cargo_count\n",
        "    logging.info(f\"Removed {cargo_count:,} cargo-only flights ({cargo_count/initial_count*100:.1f}%)\")\n",
        "    logging.info(f\"Remaining rows: {len(df_clean):,}\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def check_passenger_data_quality(df, summary):\n",
        "    \"\"\"Check for missing and anomalous passenger counts\"\"\"\n",
        "    logging.info(\"Step 2: Checking passenger data quality...\")\n",
        "\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Check for missing passenger data\n",
        "    missing_passengers = df['PASSENGERS'].isna().sum()\n",
        "    if missing_passengers > 0:\n",
        "        logging.warning(f\"Found {missing_passengers:,} rows with missing passenger data\")\n",
        "        df = df.dropna(subset=['PASSENGERS'])\n",
        "        summary['missing_passengers_removed'] = missing_passengers\n",
        "\n",
        "    # Check for negative passenger counts\n",
        "    negative_passengers = (df['PASSENGERS'] < 0).sum()\n",
        "    if negative_passengers > 0:\n",
        "        logging.warning(f\"Found {negative_passengers:,} rows with negative passenger counts\")\n",
        "        df = df[df['PASSENGERS'] >= 0]\n",
        "        summary['anomalous_passengers_removed'] += negative_passengers\n",
        "\n",
        "    # Check for extremely high passenger counts (potential data errors)\n",
        "    passenger_threshold = df['PASSENGERS'].quantile(0.999)\n",
        "    extreme_passengers = (df['PASSENGERS'] > passenger_threshold).sum()\n",
        "\n",
        "    if extreme_passengers > 0:\n",
        "        logging.info(f\"Found {extreme_passengers:,} rows with extremely high passenger counts (>{passenger_threshold:,.0f})\")\n",
        "        logging.info(\"These will be flagged but not removed (could be legitimate large aircraft)\")\n",
        "        summary['data_quality_issues'].append(f\"Extreme passenger counts: {extreme_passengers:,} rows\")\n",
        "\n",
        "    # Passenger statistics\n",
        "    passenger_stats = df['PASSENGERS'].describe()\n",
        "    logging.info(\"Passenger count statistics:\")\n",
        "    logging.info(f\"  Mean: {passenger_stats['mean']:,.0f}\")\n",
        "    logging.info(f\"  Median: {passenger_stats['50%']:,.0f}\")\n",
        "    logging.info(f\"  Max: {passenger_stats['max']:,.0f}\")\n",
        "\n",
        "    removed_count = initial_count - len(df)\n",
        "    if removed_count > 0:\n",
        "        logging.info(f\"Removed {removed_count:,} rows with data quality issues\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def filter_domestic_routes(df, summary):\n",
        "    \"\"\"Filter for domestic U.S. routes only\"\"\"\n",
        "    logging.info(\"Step 3: Filtering for domestic U.S. routes...\")\n",
        "\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Check available country codes\n",
        "    if 'ORIGIN_COUNTRY' not in df.columns or 'DEST_COUNTRY' not in df.columns:\n",
        "        raise KeyError(\"Expected columns 'ORIGIN_COUNTRY' and 'DEST_COUNTRY' not found in dataset.\")\n",
        "\n",
        "    origin_countries = df['ORIGIN_COUNTRY'].value_counts()\n",
        "    dest_countries = df['DEST_COUNTRY'].value_counts()\n",
        "\n",
        "    logging.info(f\"Origin countries found: {list(origin_countries.index)}\")\n",
        "    logging.info(f\"Destination countries found: {list(dest_countries.index)}\")\n",
        "\n",
        "    # Filter for domestic routes (both origin and destination in US)\n",
        "    domestic_mask = (df['ORIGIN_COUNTRY'] == 'US') & (df['DEST_COUNTRY'] == 'US')\n",
        "    df_domestic = df[domestic_mask].copy()\n",
        "\n",
        "    removed_count = initial_count - len(df_domestic)\n",
        "    summary['non_domestic_removed'] = removed_count\n",
        "\n",
        "    logging.info(f\"Removed {removed_count:,} non-domestic routes ({removed_count/initial_count*100:.1f}%)\")\n",
        "    logging.info(f\"Remaining domestic routes: {len(df_domestic):,}\")\n",
        "\n",
        "    return df_domestic\n",
        "\n",
        "def remove_duplicates(df, summary):\n",
        "    \"\"\"Remove duplicate route entries\"\"\"\n",
        "    logging.info(\"Step 4: Removing duplicate entries...\")\n",
        "\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Define key columns that identify a unique route-time combination\n",
        "    key_columns = [\n",
        "        'YEAR', 'QUARTER', 'MONTH',\n",
        "        'UNIQUE_CARRIER', 'ORIGIN', 'DEST',\n",
        "        'CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID'\n",
        "    ]\n",
        "\n",
        "    existing_key_columns = [col for col in key_columns if col in df.columns]\n",
        "    logging.info(f\"Using columns for duplicate detection: {existing_key_columns}\")\n",
        "\n",
        "    duplicate_mask = df.duplicated(subset=existing_key_columns, keep='first')\n",
        "    duplicate_count = duplicate_mask.sum()\n",
        "\n",
        "    if duplicate_count > 0:\n",
        "        logging.warning(f\"Found {duplicate_count:,} duplicate entries\")\n",
        "        sample_duplicates = df[duplicate_mask].head()\n",
        "        logging.info(\"Sample duplicate entries:\")\n",
        "        for _, row in sample_duplicates.iterrows():\n",
        "            try:\n",
        "                logging.info(f\"  {row.get('YEAR','?')}-Q{row.get('QUARTER','?')} {row.get('ORIGIN','?')}->{row.get('DEST','?')} {row.get('UNIQUE_CARRIER','?')}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    df_clean = df[~duplicate_mask].copy()\n",
        "\n",
        "    summary['duplicates_removed'] = duplicate_count\n",
        "    logging.info(f\"Removed {duplicate_count:,} duplicate entries\")\n",
        "    logging.info(f\"Remaining rows: {len(df_clean):,}\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def handle_seasonal_variations(df):\n",
        "    \"\"\"Aggregate data quarterly and add seasonal analysis\"\"\"\n",
        "    logging.info(\"Step 5: Handling seasonal variations...\")\n",
        "\n",
        "    # Ensure QUARTER column exists and is valid\n",
        "    if 'QUARTER' in df.columns:\n",
        "        valid_quarters = df['QUARTER'].between(1, 4)\n",
        "        invalid_quarters = (~valid_quarters).sum()\n",
        "\n",
        "        if invalid_quarters > 0:\n",
        "            logging.warning(f\"Found {invalid_quarters:,} rows with invalid quarter values\")\n",
        "            df = df[valid_quarters]\n",
        "\n",
        "    # Create quarterly aggregation\n",
        "    group_cols = ['YEAR', 'QUARTER', 'ORIGIN', 'DEST']\n",
        "    agg_map = {\n",
        "        'PASSENGERS': 'sum',\n",
        "        'FREIGHT': 'sum',\n",
        "        'MAIL': 'sum',\n",
        "        'DISTANCE': 'mean',\n",
        "    }\n",
        "    # Add optional columns if present\n",
        "    optional_first = ['UNIQUE_CARRIER', 'ORIGIN_CITY_NAME', 'DEST_CITY_NAME', 'ORIGIN_STATE_ABR', 'DEST_STATE_ABR']\n",
        "    for col in optional_first:\n",
        "        if col in df.columns:\n",
        "            agg_map[col] = 'first'\n",
        "\n",
        "    quarterly_agg = df.groupby([c for c in group_cols if c in df.columns]).agg(agg_map).reset_index()\n",
        "\n",
        "    logging.info(f\"Quarterly aggregation created: {len(quarterly_agg):,} route-quarter combinations\")\n",
        "\n",
        "    # Calculate seasonal patterns\n",
        "    if 'QUARTER' in quarterly_agg.columns and 'PASSENGERS' in quarterly_agg.columns:\n",
        "        seasonal_stats = quarterly_agg.groupby('QUARTER')['PASSENGERS'].agg(['sum', 'mean', 'count'])\n",
        "        logging.info(\"Seasonal passenger patterns:\")\n",
        "        for quarter in [1, 2, 3, 4]:\n",
        "            if quarter in seasonal_stats.index:\n",
        "                stats = seasonal_stats.loc[quarter]\n",
        "                logging.info(f\"  Q{quarter}: {stats['sum']:,} total passengers, {stats['mean']:,.0f} avg per route\")\n",
        "\n",
        "    return df, quarterly_agg\n",
        "\n",
        "def generate_cleanup_report(summary, output_dir='.'):\n",
        "    \"\"\"Generate comprehensive cleanup report\"\"\"\n",
        "    logging.info(\"Generating cleanup report...\")\n",
        "\n",
        "    # Calculate percentages\n",
        "    total_removed = (summary['cargo_only_removed'] +\n",
        "                     summary['missing_passengers_removed'] +\n",
        "                     summary['anomalous_passengers_removed'] +\n",
        "                     summary['non_domestic_removed'] +\n",
        "                     summary['duplicates_removed'])\n",
        "\n",
        "    reduction_rate = (total_removed / summary['original_rows']) * 100 if summary['original_rows'] > 0 else 0\n",
        "\n",
        "    # Create report\n",
        "    report = f\"\"\"\n",
        "T-100 DATA CLEANUP REPORT\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "SUMMARY:\n",
        "========\n",
        "Original dataset size:        {summary['original_rows']:,} rows\n",
        "Final dataset size:           {summary['final_rows']:,} rows\n",
        "Total rows removed:           {total_removed:,} rows\n",
        "Data reduction:               {reduction_rate:.1f}%\n",
        "\n",
        "CLEANUP OPERATIONS:\n",
        "==================\n",
        "1. Cargo-only flights removed:       {summary['cargo_only_removed']:,} rows\n",
        "2. Missing passenger data removed:   {summary['missing_passengers_removed']:,} rows\n",
        "3. Anomalous passenger data removed: {summary['anomalous_passengers_removed']:,} rows\n",
        "4. Non-domestic routes removed:      {summary['non_domestic_removed']:,} rows\n",
        "5. Duplicate entries removed:        {summary['duplicates_removed']:,} rows\n",
        "\n",
        "DATA QUALITY ISSUES IDENTIFIED:\n",
        "==============================\n",
        "\"\"\"\n",
        "    for issue in summary['data_quality_issues']:\n",
        "        report += f\"- {issue}\\n\"\n",
        "\n",
        "    report_path = f\"{output_dir}/t100_cleanup_report.txt\"\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    logging.info(f\"Cleanup report saved to: {report_path}\")\n",
        "    print(report)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main cleanup execution function\"\"\"\n",
        "    start_time = datetime.now()\n",
        "    logging.info(\"Starting T-100 data cleanup process...\")\n",
        "\n",
        "    # Initialize summary\n",
        "    summary = setup_cleanup_summary()\n",
        "\n",
        "    try:\n",
        "        # File paths - Updated to use specified directory\n",
        "        t100_file = f\"{DATA_DIRECTORY}consolidated_t100_data.csv\"\n",
        "        output_dir = DATA_DIRECTORY\n",
        "\n",
        "        logging.info(f\"Data directory: {DATA_DIRECTORY}\")\n",
        "        logging.info(f\"T-100 file: {t100_file}\")\n",
        "\n",
        "        # Check if required file exists\n",
        "        if not os.path.exists(t100_file):\n",
        "            raise FileNotFoundError(f\"T-100 data file not found: {t100_file}\")\n",
        "\n",
        "        logging.info(\"‚úÖ Required file found\")\n",
        "\n",
        "        # Load T-100 data\n",
        "        df = load_t100_data(t100_file)\n",
        "        summary['original_rows'] = len(df)\n",
        "\n",
        "        logging.info(f\"Original data columns: {list(df.columns)}\")\n",
        "\n",
        "        # Cleanup pipeline (airport-code validation intentionally skipped)\n",
        "        df = cleanup_cargo_only_flights(df, summary)\n",
        "        df = check_passenger_data_quality(df, summary)\n",
        "        df = filter_domestic_routes(df, summary)\n",
        "        df = remove_duplicates(df, summary)\n",
        "\n",
        "        # Seasonal handling / quarterly aggregation\n",
        "        df_clean, quarterly_agg = handle_seasonal_variations(df)\n",
        "\n",
        "        summary['final_rows'] = len(df_clean)\n",
        "\n",
        "        # Save cleaned data\n",
        "        output_file = f\"{output_dir}consolidated_t100_data_cleaned.csv\"\n",
        "        quarterly_file = f\"{output_dir}t100_quarterly_aggregated.csv\"\n",
        "\n",
        "        logging.info(f\"Saving cleaned data to {output_file}...\")\n",
        "        df_clean.to_csv(output_file, index=False)\n",
        "\n",
        "        logging.info(f\"Saving quarterly aggregated data to {quarterly_file}...\")\n",
        "        quarterly_agg.to_csv(quarterly_file, index=False)\n",
        "\n",
        "        # Generate report\n",
        "        generate_cleanup_report(summary, output_dir)\n",
        "\n",
        "        # Final statistics\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        logging.info(f\"Cleanup completed successfully in {duration}\")\n",
        "        logging.info(f\"Clean dataset: {len(df_clean):,} rows\")\n",
        "        logging.info(f\"Quarterly aggregated dataset: {len(quarterly_agg):,} rows\")\n",
        "\n",
        "        return df_clean, quarterly_agg, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during cleanup process: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the cleanup process\n",
        "    cleaned_data, quarterly_data, cleanup_summary = main()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"T-100 DATA CLEANUP COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Files created in /content/drive/MyDrive/airline_data_analysis_v2/:\")\n",
        "    print(f\"- consolidated_t100_data_cleaned.csv\")\n",
        "    print(f\"- t100_quarterly_aggregated.csv\")\n",
        "    print(f\"- t100_cleanup_report.txt\")\n",
        "    print(f\"- t100_cleanup.log\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AX6nf9HUMDW",
        "outputId": "489dd6ab-62a4-404b-ef18-8ba5a8fce22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Found 17,899 duplicate entries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "T-100 DATA CLEANUP REPORT\n",
            "Generated: 2025-08-29 21:18:51\n",
            "\n",
            "SUMMARY:\n",
            "========\n",
            "Original dataset size:        1,291,216 rows\n",
            "Final dataset size:           815,641 rows\n",
            "Total rows removed:           475,575 rows\n",
            "Data reduction:               36.8%\n",
            "\n",
            "CLEANUP OPERATIONS:\n",
            "==================\n",
            "1. Cargo-only flights removed:       229,336 rows\n",
            "2. Missing passenger data removed:   0 rows\n",
            "3. Anomalous passenger data removed: 0 rows\n",
            "4. Non-domestic routes removed:      228,340 rows\n",
            "5. Duplicate entries removed:        17,899 rows\n",
            "\n",
            "DATA QUALITY ISSUES IDENTIFIED:\n",
            "==============================\n",
            "- Extreme passenger counts: 1,061 rows\n",
            "\n",
            "\n",
            "============================================================\n",
            "T-100 DATA CLEANUP COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Files created in /content/drive/MyDrive/airline_data_analysis_v2/:\n",
            "- consolidated_t100_data_cleaned.csv\n",
            "- t100_quarterly_aggregated.csv\n",
            "- t100_cleanup_report.txt\n",
            "- t100_cleanup.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Extract all US airports from the T-100 dataset\n",
        "include all origin and destination airports"
      ],
      "metadata": {
        "id": "dcRn2GIPiEN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIRECTORY = \"/content/drive/MyDrive/airline_data_analysis_v2/\"  # <-- CHANGE THIS IF NEEDED\n",
        "consolidated_file = Path(DATA_DIRECTORY) / \"consolidated_t100_data_cleaned.csv\"\n",
        "unique_airports_t100 = Path(DATA_DIRECTORY) / \"unique_airports_t100.csv\"\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(consolidated_file, dtype=str)  # keep as string to avoid type issues\n",
        "\n",
        "# Helper to pick the first matching column name if present\n",
        "def pick(*names):\n",
        "    return next((c for c in names if c in df.columns), None)\n",
        "\n",
        "# Sanity check for core columns\n",
        "if 'ORIGIN' not in df.columns or 'DEST' not in df.columns:\n",
        "    raise KeyError(\"Expected columns 'ORIGIN' and 'DEST' not found in the cleaned T-100 file.\")\n",
        "\n",
        "# Candidate columns (some datasets have slightly different names)\n",
        "origin_id_col   = pick('ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_ID')\n",
        "origin_city_col = pick('ORIGIN_CITY_NAME', 'ORIGIN_CITY')\n",
        "origin_ctry_col = pick('ORIGIN_COUNTRY')\n",
        "\n",
        "dest_id_col     = pick('DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEST_ID')\n",
        "dest_city_col   = pick('DEST_CITY_NAME', 'DEST_CITY')\n",
        "dest_ctry_col   = pick('DEST_COUNTRY')\n",
        "\n",
        "# Build origin airports frame\n",
        "origin_rename = {\n",
        "    **({origin_id_col: 'AIRPORT_ID'} if origin_id_col else {}),\n",
        "    'ORIGIN': 'AIRPORT_NAME',\n",
        "    **({origin_city_col: 'CITY'} if origin_city_col else {}),\n",
        "    **({origin_ctry_col: 'COUNTRY'} if origin_ctry_col else {}),\n",
        "}\n",
        "origin_airports = df[list(origin_rename.keys())].rename(columns=origin_rename)\n",
        "\n",
        "# Build destination airports frame\n",
        "dest_rename = {\n",
        "    **({dest_id_col: 'AIRPORT_ID'} if dest_id_col else {}),\n",
        "    'DEST': 'AIRPORT_NAME',\n",
        "    **({dest_city_col: 'CITY'} if dest_city_col else {}),\n",
        "    **({dest_ctry_col: 'COUNTRY'} if dest_ctry_col else {}),\n",
        "}\n",
        "dest_airports = df[list(dest_rename.keys())].rename(columns=dest_rename)\n",
        "\n",
        "# Combine, normalize, and dedupe\n",
        "all_airports = pd.concat([origin_airports, dest_airports], ignore_index=True)\n",
        "\n",
        "# Clean up whitespace/case for safety\n",
        "for col in ['AIRPORT_ID', 'AIRPORT_NAME', 'CITY', 'COUNTRY']:\n",
        "    if col in all_airports.columns:\n",
        "        all_airports[col] = all_airports[col].astype(str).str.strip()\n",
        "\n",
        "# Prefer uppercase codes for AIRPORT_NAME if they‚Äôre IATA/FAA codes\n",
        "if 'AIRPORT_NAME' in all_airports.columns:\n",
        "    all_airports['AIRPORT_NAME'] = all_airports['AIRPORT_NAME'].str.upper()\n",
        "\n",
        "# Create a dedup key: use AIRPORT_ID if available, else fall back to AIRPORT_NAME\n",
        "dedup_key = all_airports['AIRPORT_ID'].where(all_airports['AIRPORT_ID'].notna() & (all_airports['AIRPORT_ID'] != ''), all_airports.get('AIRPORT_NAME'))\n",
        "all_airports['__DEDUP_KEY__'] = dedup_key\n",
        "\n",
        "# Drop duplicates on the key\n",
        "all_airports = all_airports.drop_duplicates(subset='__DEDUP_KEY__').drop(columns='__DEDUP_KEY__')\n",
        "\n",
        "# Sort (by ID if present, else by name)\n",
        "if 'AIRPORT_ID' in all_airports.columns:\n",
        "    all_airports = all_airports.sort_values(by=['AIRPORT_ID', 'AIRPORT_NAME'])\n",
        "else:\n",
        "    all_airports = all_airports.sort_values(by=['AIRPORT_NAME'])\n",
        "\n",
        "# Optional: filter to US only (the cleaned file should already be domestic-only, so usually unnecessary)\n",
        "# if 'COUNTRY' in all_airports.columns:\n",
        "#     all_airports = all_airports[all_airports['COUNTRY'].str.upper() == 'US']\n",
        "\n",
        "# Reset index\n",
        "all_airports = all_airports.reset_index(drop=True)\n",
        "\n",
        "# Save to CSV\n",
        "all_airports.to_csv(unique_airports_t100, index=False)\n",
        "\n",
        "print(f\"Total unique airports found: {len(all_airports)}\")\n",
        "print(all_airports.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGBiAKuhiJES",
        "outputId": "e0556808-d62c-432d-f2c7-e529f88b93e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique airports found: 1636\n",
            "  AIRPORT_ID AIRPORT_NAME              CITY COUNTRY\n",
            "0      10001          01A  Afognak Lake, AK      US\n",
            "1      10005          05A  Little Squaw, AK      US\n",
            "2      10006          06A      Kizhuyak, AK      US\n",
            "3      10009          09A         Homer, AK      US\n",
            "4      10010          1B1        Hudson, NY      US\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3.A - Prepare for Step 5\n",
        "- Find the unique list of cities and state combination from the unique_airports_t100.csv\n",
        "- Check how many are not present in the file \"city_business_leisure_classification_detailed.csv\""
      ],
      "metadata": {
        "id": "WqfMqjR_2W2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIRECTORY = \"/content/drive/MyDrive/airline_data_analysis_v2/\"\n",
        "unique_airports_file = Path(DATA_DIRECTORY) / \"unique_airports_t100.csv\"\n",
        "city_classification_file = Path(DATA_DIRECTORY) / \"city_business_leisure_classification_detailed.csv\"\n",
        "out_csv = Path(DATA_DIRECTORY) / \"unmatched_city_state_combos.csv\"\n",
        "\n",
        "# Load\n",
        "df_airports = pd.read_csv(unique_airports_file, dtype=str)\n",
        "df_class = pd.read_csv(city_classification_file, dtype=str)\n",
        "\n",
        "# Split CITY into City / State (robust to missing comma)\n",
        "split = df_airports[\"CITY\"].fillna(\"\").str.rsplit(\",\", n=1, expand=True)\n",
        "df_airports[\"City\"] = split[0].str.strip()\n",
        "df_airports[\"State\"] = split[1].str.strip()\n",
        "\n",
        "# Normalize for join\n",
        "df_airports[\"City_norm\"] = df_airports[\"City\"].str.lower()\n",
        "df_airports[\"State_norm\"] = df_airports[\"State\"].str.lower()\n",
        "df_class[\"City_norm\"] = df_class[\"City\"].astype(str).str.strip().str.lower()\n",
        "df_class[\"State_norm\"] = df_class[\"State/Territory\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Left join to find city+state in airports not present in classification\n",
        "merged = df_airports.merge(\n",
        "    df_class[[\"City_norm\", \"State_norm\"]].drop_duplicates(),\n",
        "    on=[\"City_norm\", \"State_norm\"],\n",
        "    how=\"left\",\n",
        "    indicator=True\n",
        ")\n",
        "\n",
        "# Unique unmatched combos\n",
        "unmatched = (merged[merged[\"_merge\"] == \"left_only\"]\n",
        "             [[\"City\", \"State\"]]\n",
        "             .dropna()\n",
        "             .drop_duplicates()\n",
        "             .sort_values([\"State\", \"City\"]))\n",
        "\n",
        "print(f\"Number of unmatched City+State combinations: {len(unmatched)}\")\n",
        "\n",
        "# Save\n",
        "unmatched.to_csv(out_csv, index=False)\n",
        "print(f\"Saved unmatched combinations to: {out_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL_CxTDu4aeT",
        "outputId": "fd760b4c-a8b3-4122-ac08-b2d1fe567176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unmatched City+State combinations: 0\n",
            "Saved unmatched combinations to: /content/drive/MyDrive/airline_data_analysis_v2/unmatched_city_state_combos.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Hub Status from 2018\n",
        "\n",
        "- To be used for the Hypothesis 1\n"
      ],
      "metadata": {
        "id": "5rc6MyhF2M7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ========= CONFIG =========\n",
        "DATA_DIRECTORY = \"/content/drive/MyDrive/airline_data_analysis_v2/\"  # <-- CHANGE IF NEEDED\n",
        "cleaned_t100_path = Path(DATA_DIRECTORY) / \"consolidated_t100_data_cleaned.csv\"\n",
        "airports_in_path  = Path(DATA_DIRECTORY) / \"unique_airports_t100.csv\"\n",
        "airports_out_path = Path(DATA_DIRECTORY) / \"unique_airports_t100_hub_updated.csv\"\n",
        "HUB_YEAR = 2018\n",
        "# =========================\n",
        "\n",
        "def classify_airport_hub(passengers, total_us_passengers):\n",
        "    \"\"\"User-provided logic: classify by share of total US passengers (%).\"\"\"\n",
        "    if total_us_passengers <= 0 or pd.isna(passengers):\n",
        "        return \"Non-Hub Primary\"\n",
        "    share = (passengers / total_us_passengers) * 100.0\n",
        "    if share >= 1.0:\n",
        "        return \"Major Hub\"\n",
        "    elif share >= 0.25:\n",
        "        return \"Medium Hub\"\n",
        "    elif share >= 0.05:\n",
        "        return \"Small Hub\"\n",
        "    else:\n",
        "        return \"Non-Hub Primary\"\n",
        "\n",
        "def pick(df, *names):\n",
        "    \"\"\"Return the first existing column name from candidates.\"\"\"\n",
        "    return next((c for c in names if c in df.columns), None)\n",
        "\n",
        "# ---------- Load data ----------\n",
        "df = pd.read_csv(cleaned_t100_path, dtype=str)\n",
        "airports_df = pd.read_csv(airports_in_path, dtype=str)\n",
        "\n",
        "# Basic checks\n",
        "if 'YEAR' not in df.columns or 'PASSENGERS' not in df.columns:\n",
        "    raise KeyError(\"Expected columns 'YEAR' and 'PASSENGERS' in the cleaned T-100 file.\")\n",
        "\n",
        "# Convert types\n",
        "df['YEAR'] = pd.to_numeric(df['YEAR'], errors='coerce').astype('Int64')\n",
        "df['PASSENGERS'] = pd.to_numeric(df['PASSENGERS'], errors='coerce')\n",
        "\n",
        "# Filter to the requested year\n",
        "df_2018 = df[df['YEAR'] == HUB_YEAR].copy()\n",
        "\n",
        "# Identify airport key columns (prefer IDs; fall back to codes)\n",
        "origin_id_col = pick(df_2018, 'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_ID')\n",
        "dest_id_col   = pick(df_2018, 'DEST_AIRPORT_ID',   'DEST_AIRPORT_SEQ_ID',   'DEST_ID')\n",
        "origin_col    = pick(df_2018, 'ORIGIN')\n",
        "dest_col      = pick(df_2018, 'DEST')\n",
        "\n",
        "if not origin_col or not dest_col:\n",
        "    raise KeyError(\"Expected 'ORIGIN' and 'DEST' columns in the cleaned T-100 file.\")\n",
        "\n",
        "# Build per-airport passenger totals for 2018 (counting both origin and destination)\n",
        "def total_by_key(key_col):\n",
        "    return (\n",
        "        df_2018.groupby(key_col, dropna=True)['PASSENGERS']\n",
        "        .sum(min_count=1)\n",
        "        .rename('PAX_2018')\n",
        "        .astype(float)\n",
        "    )\n",
        "\n",
        "if origin_id_col and dest_id_col:\n",
        "    origin_tot = total_by_key(origin_id_col)\n",
        "    dest_tot   = total_by_key(dest_id_col)\n",
        "    pax_by_airport = origin_tot.add(dest_tot, fill_value=0.0)\n",
        "    pax_by_airport.index.name = 'AIRPORT_ID'\n",
        "    key_for_merge = 'AIRPORT_ID'\n",
        "    airports_df['AIRPORT_ID'] = airports_df.get('AIRPORT_ID', '').astype(str).str.strip()\n",
        "    pax_df = pax_by_airport.reset_index()\n",
        "    pax_df['AIRPORT_ID'] = pax_df['AIRPORT_ID'].astype(str).str.strip()\n",
        "else:\n",
        "    origin_tot = total_by_key(origin_col)\n",
        "    dest_tot   = total_by_key(dest_col)\n",
        "    pax_by_airport = origin_tot.add(dest_tot, fill_value=0.0)\n",
        "    pax_by_airport.index.name = 'AIRPORT_NAME'\n",
        "    key_for_merge = 'AIRPORT_NAME'\n",
        "    if 'AIRPORT_NAME' not in airports_df.columns:\n",
        "        raise KeyError(\"Fallback requires 'AIRPORT_NAME' in unique_airports_t100.csv.\")\n",
        "    airports_df['AIRPORT_NAME'] = airports_df['AIRPORT_NAME'].astype(str).str.upper().str.strip()\n",
        "    pax_df = pax_by_airport.reset_index()\n",
        "    pax_df['AIRPORT_NAME'] = pax_df['AIRPORT_NAME'].astype(str).str.upper().str.strip()\n",
        "\n",
        "# National total and shares\n",
        "total_us_pax_2018 = pax_df['PAX_2018'].sum()\n",
        "pax_df['US_SHARE_2018_PCT'] = (pax_df['PAX_2018'] / total_us_pax_2018 * 100.0).round(6)\n",
        "pax_df['HUB_TYPE'] = pax_df['PAX_2018'].apply(lambda x: classify_airport_hub(x, total_us_pax_2018))\n",
        "pax_df['HUB_YEAR'] = HUB_YEAR\n",
        "\n",
        "# Merge into the airports table\n",
        "cols_to_add = ['PAX_2018', 'US_SHARE_2018_PCT', 'HUB_TYPE', 'HUB_YEAR']\n",
        "merged = airports_df.merge(pax_df[[key_for_merge] + cols_to_add], on=key_for_merge, how='left')\n",
        "\n",
        "# >>> NEW: mark airports not present in 2018 as NEW in HUB_TYPE\n",
        "merged['HUB_TYPE'] = merged['HUB_TYPE'].fillna('NEW')\n",
        "\n",
        "# (Optional) If you also want to zero-fill metrics for NEW airports, uncomment:\n",
        "merged['PAX_2018'] = merged['PAX_2018'].fillna(0.0)\n",
        "merged['US_SHARE_2018_PCT'] = merged['US_SHARE_2018_PCT'].fillna(0.0)\n",
        "\n",
        "# Sort & save\n",
        "sort_cols = [c for c in ['AIRPORT_ID', 'AIRPORT_NAME'] if c in merged.columns]\n",
        "if sort_cols:\n",
        "    merged = merged.sort_values(by=sort_cols).reset_index(drop=True)\n",
        "\n",
        "merged.to_csv(airports_out_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Hub classification completed for {HUB_YEAR}.\")\n",
        "print(f\"   Total US passengers counted (both ends): {total_us_pax_2018:,.0f}\")\n",
        "print(\"   Hub-type counts (including NEW):\")\n",
        "print(merged['HUB_TYPE'].value_counts(dropna=False))\n",
        "print(f\"\\nüíæ Saved: {airports_out_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DET7P7RR2VhI",
        "outputId": "1994896d-f49d-46a0-9a26-c7598838723c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Hub classification completed for 2018.\n",
            "   Total US passengers counted (both ends): 1,518,702,636\n",
            "   Hub-type counts (including NEW):\n",
            "HUB_TYPE\n",
            "Non-Hub Primary    1050\n",
            "NEW                 446\n",
            "Small Hub            71\n",
            "Medium Hub           40\n",
            "Major Hub            29\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üíæ Saved: /content/drive/MyDrive/airline_data_analysis_v2/unique_airports_t100_hub_updated.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - creates \"consolidated_t100_data_cleaned_tp_updated.csv\" by assigning travel purpose classifications to every row in the consolidated T-100 dataset\n",
        "\n",
        "- To be used for the Hypothesis 2\n"
      ],
      "metadata": {
        "id": "XG9kit3me5Ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5 (Alternative)"
      ],
      "metadata": {
        "id": "ONAgqTKT75IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute_tpi_for_t100_city_state.py  (CITY_STATE-based matching + diagnostics)\n",
        "from pathlib import Path\n",
        "import re, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# ========= File locations =========\n",
        "DATA_DIRECTORY = Path(\"/content/drive/MyDrive/airline_data_analysis_v2/\")\n",
        "T100_PATH   = DATA_DIRECTORY / \"consolidated_t100_data_cleaned.csv\"\n",
        "CLASS_PATH  = DATA_DIRECTORY / \"city_business_leisure_classification_detailed.csv\"\n",
        "OUT_PATH    = DATA_DIRECTORY / \"consolidated_t100_with_tpi_city_state.csv\"\n",
        "MISSING_OUT = DATA_DIRECTORY / \"missing_consolidated_t100_with_tpi_city_state.csv\"\n",
        "\n",
        "# Diagnostics outputs\n",
        "UNMATCHED_ORIG_COUNTS = DATA_DIRECTORY / \"unmatched_origin_city_state_counts.csv\"\n",
        "UNMATCHED_DEST_COUNTS = DATA_DIRECTORY / \"unmatched_dest_city_state_counts.csv\"\n",
        "\n",
        "# ========= Batch settings =========\n",
        "CHUNK_SIZE = 250_000\n",
        "\n",
        "# ========= Model settings =========\n",
        "DESTINATION_WEIGHT = 0.6\n",
        "THRESHOLDS = (40.0, 60.0)\n",
        "ORIG_MULT = 0.8\n",
        "DEST_MULT = 1.0\n",
        "DEST_SEASON_BONUS = 1.1\n",
        "CONF_WEIGHTS = (0.6, 0.4)\n",
        "\n",
        "MARKET_DELTAS = { \"Market_Military/Defense_Base\": +12, \"Market_Military_Test/Eval_Range\": +10, \"Market_StateCapital/Government\": +8, \"Market_NorthSlope_Oilfield\": +15, \"Market_Mining\": +12, \"Market_Remote/EAS\": +10, \"Market_Fishing/Cannery\": +8, \"Market_AlaskaRegionalHub\": +5, \"Market_DiversifiedMetro/Corporate\": +5, \"Market_RegionalServices/Healthcare/LightIndustry\": +4, \"Market_Bizjet_Hub\": +8, \"Market_Border/Port_Logistics\": +6, \"Market_Port/Shipbuilding/Shipyards\": +5, \"Market_Port/Coastal_Mixed\": +2, \"Market_CruiseGateway/Port\": -8, \"Market_ParkGateway\": -8, \"Market_WildernessLodge/Adventure\": -10, \"Market_ResortIsland\": -12, \"Market_CoastalTourism/Golf/Aquarium\": -6, \"Market_Wine/Culinary\": -6, \"Market_Weekender/HistoricTown\": -4, \"Market_Ski/Outdoor_Gateway\": -6, \"Market_Ferry_Gateway\": -3, \"Market_Island/Interisland_Essential\": -3, }\n",
        "LEXICAL_NEG_DELTAS = { \"Market_LexicalCue_Springs\": -2, \"Market_LexicalCue_Lake\": -2, \"Market_LexicalCue_Valley\": -2, \"Market_LexicalCue_Park\": -2, \"Market_LexicalCue_Ocean\": -2, \"Market_LexicalCue_Harbor\": -2, \"Market_LexicalCue_Island\": -2, }\n",
        "LEXICAL_CAP = -5.0\n",
        "SEASON_DELTAS = { \"Season_SummerPeaks\": -6, \"Season_WeekendHeavy\": -4, \"Season_Festivals/Events\": -4, \"Season_Snowbird\": -4, \"Season_ShoulderSeason\": -2, \"Season_SkiSeason\": -8, \"Season_ParkSeason\": -6, \"Season_CruiseSeason\": -6, \"Season_Weekender/SecondHomes\": -4, }\n",
        "\n",
        "REQUIRED_T100_COLS = {\n",
        "    \"ORIGIN_CITY_NAME\",\n",
        "    \"DEST_CITY_NAME\",\n",
        "    \"MONTH\"\n",
        "}\n",
        "\n",
        "def _strip_accents(s: str) -> str:\n",
        "    try:\n",
        "        import unicodedata\n",
        "        return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n",
        "    except Exception:\n",
        "        return s\n",
        "\n",
        "def slug_city_state(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Regex-style normalization used for matching:\n",
        "    - lowercase, strip accents\n",
        "    - remove all non [a-z0-9]\n",
        "    This lets 'Austin, TX' match 'austin, tx' regardless of spaces/special chars.\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = _strip_accents(s.strip().lower())\n",
        "    return re.sub(r\"[^a-z0-9]\", \"\", s)\n",
        "\n",
        "def find_cols(df, targets):\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    for t in targets:\n",
        "        for lc, real in cols.items():\n",
        "            if t in lc:\n",
        "                return real\n",
        "    return None\n",
        "\n",
        "def ensure_col(df, col):\n",
        "    if col not in df.columns:\n",
        "        df[col] = 0\n",
        "\n",
        "def enrich_overrides_from_text(cf: pd.DataFrame, city_col: str, why_col: str | None, primary_col: str | None):\n",
        "    # Build combined lowercase text\n",
        "    if (why_col and why_col in cf.columns) or (primary_col and primary_col in cf.columns):\n",
        "        txt = pd.Series(\"\", index=cf.index, dtype=\"object\")\n",
        "        if primary_col and primary_col in cf.columns:\n",
        "            txt = txt.str.cat(cf[primary_col].astype(str), sep=\" \")\n",
        "        if why_col and why_col in cf.columns:\n",
        "            txt = txt.str.cat(cf[why_col].astype(str), sep=\" \")\n",
        "        txt = txt.str.lower().fillna(\"\")\n",
        "    else:\n",
        "        txt = pd.Series(\"\", index=cf.index, dtype=\"object\")\n",
        "\n",
        "    def mark(col, cond):\n",
        "        ensure_col(cf, col)\n",
        "        cf[col] = np.where(cond | (cf[col].astype(int) == 1), 1, 0)\n",
        "\n",
        "    # Non-capturing groups + na=False to avoid warnings\n",
        "    mark(\"Market_ResortIsland\",             txt.str.contains(r\"\\bresort\\b|beach|coast|coastal|island\", na=False))\n",
        "    mark(\"Market_ParkGateway\",              txt.str.contains(r\"national park|park gateway\", na=False))\n",
        "    mark(\"Market_CruiseGateway/Port\",       txt.str.contains(r\"\\bcruise\\b\", na=False))\n",
        "    mark(\"Market_Ski/Outdoor_Gateway\",      txt.str.contains(r\"\\bski\", na=False))\n",
        "    mark(\"Market_Military/Defense_Base\",    txt.str.contains(r\"\\b(?:afb|nas|mcas|joint base|naval|air force|usaf|usmc|army|fort )\\b\", na=False))\n",
        "    mark(\"Market_Military_Test/Eval_Range\", txt.str.contains(r\"(?:weapons station|flight test|fighter wing|test & evaluation)\", na=False))\n",
        "    mark(\"Market_StateCapital/Government\",  txt.str.contains(r\"(?:state capital|legislat)\", na=False))\n",
        "    mark(\"Market_NorthSlope_Oilfield\",      txt.str.contains(r\"(?:north slope|pipeline|pads|camps)\", na=False))\n",
        "    mark(\"Market_Mining\",                   txt.str.contains(r\"\\b(?:mine|mining)\", na=False))\n",
        "    mark(\"Market_Remote/EAS\",               txt.str.contains(r\"\\bremote\\b|essential air service|\\beas\\b\", na=False))\n",
        "    mark(\"Market_Fishing/Cannery\",          txt.str.contains(r\"(?:fishing|cannery|seafood|tender)\", na=False))\n",
        "    mark(\"Market_AlaskaRegionalHub\",        txt.str.contains(r\"(?:alaska regional hub)\", na=False))\n",
        "    mark(\"Market_DiversifiedMetro/Corporate\", txt.str.contains(r\"(?:diversified metro|corporate|manufacturing|logistics|services base)\", na=False))\n",
        "    mark(\"Market_RegionalServices/Healthcare/LightIndustry\", txt.str.contains(r\"(?:regional services|healthcare|light industry)\", na=False))\n",
        "    mark(\"Market_Bizjet_Hub\",               txt.str.contains(r\"(?:bizjet|business aviation|private jet|fbo)\", na=False))\n",
        "    mark(\"Market_Border/Port_Logistics\",    txt.str.contains(r\"(?:border|port logistics)\", na=False))\n",
        "    mark(\"Market_Port/Shipbuilding/Shipyards\", txt.str.contains(r\"(?:shipbuilding|shipyard|deepwater port|port economy)\", na=False))\n",
        "    mark(\"Market_CoastalTourism/Golf/Aquarium\", txt.str.contains(r\"(?:coastal leisure|golf|aquarium)\", na=False))\n",
        "    mark(\"Market_Wine/Culinary\",            txt.str.contains(r\"(?:wine|culinary|tasting)\", na=False))\n",
        "    mark(\"Market_Weekender/HistoricTown\",   txt.str.contains(r\"(?:weekender|historic town|heritage)\", na=False))\n",
        "\n",
        "    # Seasons\n",
        "    mark(\"Season_ParkSeason\",               txt.str.contains(r\"national park|park gateway\", na=False))\n",
        "    mark(\"Season_CruiseSeason\",             txt.str.contains(r\"\\bcruise\\b\", na=False))\n",
        "    mark(\"Season_SkiSeason\",                txt.str.contains(r\"\\bski\", na=False))\n",
        "    mark(\"Season_SummerPeaks\",              txt.str.contains(r\"(?:summer|resort|beach|coast|coastal|island|strong weekend/seasonal spikes)\", na=False))\n",
        "    mark(\"Season_ShoulderSeason\",           txt.str.contains(r\"(?:shoulder-season|foliage|spring/fall)\", na=False))\n",
        "    mark(\"Season_Snowbird\",                 txt.str.contains(r\"snowbird\", na=False))\n",
        "\n",
        "    # University\n",
        "    uni_any = txt.str.contains(r\"\\buniversity\\b|college|athletics|home game|graduation\", na=False)\n",
        "    canonical_uni = \"Season_University(HomeGames/Graduations/TermWeekdays)\"\n",
        "    ensure_col(cf, canonical_uni)\n",
        "    cf[canonical_uni] = np.where(uni_any | (cf[canonical_uni].astype(int) == 1), 1, 0)\n",
        "\n",
        "    # Conventions / legislative / quarter-end\n",
        "    mark(\"Season_Conventions/Legislative/QuarterEnd\", txt.str.contains(r\"(?:convention|legislat|capital)\", na=False))\n",
        "\n",
        "def load_classification(path: Path):\n",
        "    cf = pd.read_csv(path)\n",
        "\n",
        "    # NEW: expect CITY_STATE present and use it for exact (normalized) matching\n",
        "    city_state_col = find_cols(cf, [\"city_state\"]) or \"CITY_STATE\"\n",
        "\n",
        "    # Figure out BTI/Confidence\n",
        "    bti_col = next((c for c in cf.columns if \"bti\" in c.lower() or (\"business\" in c.lower() and \"%\" in c.lower())), None)\n",
        "    if bti_col is None:\n",
        "        raise ValueError(\"BTI column not found in classification file.\")\n",
        "    conf_col = next((c for c in cf.columns if \"confidence\" in c.lower()), None)\n",
        "    if conf_col is None:\n",
        "        raise ValueError(\"Confidence column not found in classification file.\")\n",
        "\n",
        "    # Optional text enrichment (uses Primary orientation / Why if present)\n",
        "    why_col     = find_cols(cf, [\"why\"])\n",
        "    primary_col = find_cols(cf, [\"primary orientation\"])\n",
        "    enrich_overrides_from_text(cf, city_col=city_state_col, why_col=why_col, primary_col=primary_col)\n",
        "\n",
        "    season_cols = [c for c in cf.columns if str(c).startswith(\"Season_\")]\n",
        "    market_cols = [c for c in cf.columns if str(c).startswith(\"Market_\")]\n",
        "\n",
        "    keep = [city_state_col, bti_col, conf_col] + season_cols + market_cols\n",
        "    cf = cf[keep].copy()\n",
        "\n",
        "    # Build normalized join key from CITY_STATE\n",
        "    cf[\"CITYSTATE_SLUG\"] = cf[city_state_col].astype(str).apply(slug_city_state)\n",
        "\n",
        "    # Dedupe by slug, keep highest confidence\n",
        "    cf[\"_CONF_NUM\"] = pd.to_numeric(cf[conf_col], errors=\"coerce\").fillna(0)\n",
        "    cf = (\n",
        "        cf.sort_values([\"CITYSTATE_SLUG\", \"_CONF_NUM\"], ascending=[True, False])\n",
        "          .drop_duplicates(\"CITYSTATE_SLUG\", keep=\"first\")\n",
        "          .drop(columns=\"_CONF_NUM\")\n",
        "    )\n",
        "\n",
        "    # Cast numerics\n",
        "    cf[\"BTI\"]  = pd.to_numeric(cf[bti_col], errors=\"coerce\")\n",
        "    cf[\"CONF\"] = pd.to_numeric(cf[conf_col], errors=\"coerce\")\n",
        "    for c in season_cols + market_cols:\n",
        "        if c in cf.columns:\n",
        "            cf[c] = pd.to_numeric(cf[c].replace(\"None\", np.nan), errors=\"coerce\").fillna(0).astype(np.int8)\n",
        "\n",
        "    out_cols = [\"CITYSTATE_SLUG\",\"BTI\",\"CONF\"] + season_cols + market_cols\n",
        "    return cf[out_cols], season_cols, market_cols\n",
        "\n",
        "def compute_tpi_city_state(t100: pd.DataFrame, class_df: pd.DataFrame, season_cols, market_cols):\n",
        "    \"\"\"\n",
        "    CITY_STATE-based matching:\n",
        "      - Build slugs from ORIGIN_CITY_NAME / DEST_CITY_NAME (strip non-alnum, lowercase)\n",
        "      - Join to classification on CITYSTATE_SLUG\n",
        "      - Compute TPI only for rows where both endpoints match\n",
        "      - Return (good_rows, missing_rows_with_reason)\n",
        "    \"\"\"\n",
        "    t = t100.copy()\n",
        "\n",
        "    # Slugs from City, ST strings (ignore spaces/special chars)\n",
        "    t[\"ORIG_CITYSTATE_SLUG\"] = t[\"ORIGIN_CITY_NAME\"].astype(str).apply(slug_city_state)\n",
        "    t[\"DEST_CITYSTATE_SLUG\"] = t[\"DEST_CITY_NAME\"].astype(str).apply(slug_city_state)\n",
        "\n",
        "    base_cols = [\"CITYSTATE_SLUG\",\"BTI\",\"CONF\"] + season_cols + market_cols\n",
        "    cf_o = class_df[base_cols].rename(columns={c: f\"ORIG_{c}\" for c in base_cols if c != \"CITYSTATE_SLUG\"})\n",
        "    cf_d = class_df[base_cols].rename(columns={c: f\"DEST_{c}\" for c in base_cols if c != \"CITYSTATE_SLUG\"})\n",
        "\n",
        "    t = t.merge(cf_o, left_on=\"ORIG_CITYSTATE_SLUG\", right_on=\"CITYSTATE_SLUG\", how=\"left\").drop(columns=[\"CITYSTATE_SLUG\"])\n",
        "    t = t.merge(cf_d, left_on=\"DEST_CITYSTATE_SLUG\", right_on=\"CITYSTATE_SLUG\", how=\"left\").drop(columns=[\"CITYSTATE_SLUG\"])\n",
        "\n",
        "    # Match status BEFORE fills\n",
        "    orig_matched = t[\"ORIG_BTI\"].notna() & t[\"ORIG_CONF\"].notna()\n",
        "    dest_matched = t[\"DEST_BTI\"].notna() & t[\"DEST_CONF\"].notna()\n",
        "    can_assign = orig_matched & dest_matched\n",
        "\n",
        "    # Coerce numerics\n",
        "    for c in [\"ORIG_BTI\",\"ORIG_CONF\",\"DEST_BTI\",\"DEST_CONF\",\"MONTH\"]:\n",
        "        t[c] = pd.to_numeric(t.get(c), errors=\"coerce\")\n",
        "\n",
        "    # Subset we can compute on\n",
        "    work = t.loc[can_assign].copy()\n",
        "    work[[\"ORIG_BTI\",\"DEST_BTI\"]] = work[[\"ORIG_BTI\",\"DEST_BTI\"]].fillna(50.0)\n",
        "    work[[\"ORIG_CONF\",\"DEST_CONF\"]] = work[[\"ORIG_CONF\",\"DEST_CONF\"]].fillna(50.0)\n",
        "\n",
        "    # --- Baseline (destination-biased, confidence-weighted) ---\n",
        "    wA, wB = 1.0 - DESTINATION_WEIGHT, DESTINATION_WEIGHT\n",
        "    denom = wA * work[\"ORIG_CONF\"] + wB * work[\"DEST_CONF\"]\n",
        "    baseline = (wA * work[\"ORIG_CONF\"] * work[\"ORIG_BTI\"] + wB * work[\"DEST_CONF\"] * work[\"DEST_BTI\"]) / denom.replace(0, np.nan)\n",
        "    baseline = baseline.fillna((work[\"ORIG_BTI\"] + work[\"DEST_BTI\"]) / 2.0)\n",
        "\n",
        "    # --- Market deltas ---\n",
        "    market_list = [m for m in MARKET_DELTAS if (f\"ORIG_{m}\" in work.columns or f\"DEST_{m}\" in work.columns)]\n",
        "    if market_list:\n",
        "        w_mkt = np.array([MARKET_DELTAS[m] for m in market_list], dtype=np.float32)\n",
        "        o_cols = [work.get(f\"ORIG_{m}\", 0).astype(np.float32).to_numpy() for m in market_list]\n",
        "        d_cols = [work.get(f\"DEST_{m}\", 0).astype(np.float32).to_numpy() for m in market_list]\n",
        "        o_mat = np.stack(o_cols, axis=1) if o_cols else np.zeros((len(work),0), dtype=np.float32)\n",
        "        d_mat = np.stack(d_cols, axis=1) if d_cols else np.zeros((len(work),0), dtype=np.float32)\n",
        "        market_delta = ((o_mat @ w_mkt) * ORIG_MULT + (d_mat @ w_mkt) * DEST_MULT) if o_mat.shape[1] > 0 else np.zeros(len(work), dtype=np.float32)\n",
        "        market_delta = pd.Series(market_delta, index=work.index, dtype=\"float32\")\n",
        "    else:\n",
        "        market_delta = pd.Series(np.zeros(len(work), dtype=np.float32), index=work.index)\n",
        "\n",
        "    # --- Lexical negatives (cap) ---\n",
        "    lex_keys = [k for k in LEXICAL_NEG_DELTAS if (f\"ORIG_{k}\" in work.columns or f\"DEST_{k}\" in work.columns)]\n",
        "    if lex_keys:\n",
        "        w_lex = np.array([LEXICAL_NEG_DELTAS[k] for k in lex_keys], dtype=np.float32)\n",
        "        olex = np.stack([work.get(f\"ORIG_{k}\", 0).astype(np.float32).to_numpy() for k in lex_keys], axis=1)\n",
        "        dlex = np.stack([work.get(f\"DEST_{k}\", 0).astype(np.float32).to_numpy() for k in lex_keys], axis=1)\n",
        "        lex_total = (olex @ w_lex) * (ORIG_MULT * 0.8) + (dlex @ w_lex) * DEST_MULT\n",
        "        lex_total = np.maximum(lex_total, LEXICAL_CAP).astype(np.float32)\n",
        "        lex_delta = pd.Series(lex_total, index=work.index, dtype=\"float32\")\n",
        "    else:\n",
        "        lex_delta = pd.Series(np.zeros(len(work), dtype=np.float32), index=work.index)\n",
        "\n",
        "    # --- Seasonality (month-aware) ---\n",
        "    m = work[\"MONTH\"].astype(int).to_numpy()\n",
        "    ctx_peak     = np.isin(m, (6,7,8))\n",
        "    ctx_shoulder = np.isin(m, (4,5,9,10))\n",
        "    ctx_ski      = np.isin(m, (12,1,2,3))\n",
        "    ctx_park     = np.isin(m, (6,7,8))\n",
        "    ctx_cruise   = np.isin(m, (5,6,7,8,9))\n",
        "    ctx_snowbird = np.isin(m, (12,1,2,3))\n",
        "    ctx_quarter  = np.isin(m, (3,6,9,12))\n",
        "    ctx_uni      = np.isin(m, (5,6,9,10,11))\n",
        "\n",
        "    def season_component(flag, active_mask, base_delta):\n",
        "        o = work.get(f\"ORIG_{flag}\", 0).astype(np.float32).to_numpy()\n",
        "        d = work.get(f\"DEST_{flag}\", 0).astype(np.float32).to_numpy()\n",
        "        delta = (o * ORIG_MULT + d * DEST_MULT * DEST_SEASON_BONUS) * base_delta\n",
        "        delta[~active_mask] = 0.0\n",
        "        return pd.Series(delta, index=work.index, dtype=\"float32\")\n",
        "\n",
        "    season_delta = pd.Series(np.zeros(len(work), dtype=np.float32), index=work.index)\n",
        "    for flag, (mask, base) in {\n",
        "        \"Season_SummerPeaks\":   (ctx_peak,     SEASON_DELTAS[\"Season_SummerPeaks\"]),\n",
        "        \"Season_ShoulderSeason\":(ctx_shoulder, SEASON_DELTAS[\"Season_ShoulderSeason\"]),\n",
        "        \"Season_SkiSeason\":     (ctx_ski,      SEASON_DELTAS[\"Season_SkiSeason\"]),\n",
        "        \"Season_ParkSeason\":    (ctx_park,     SEASON_DELTAS[\"Season_ParkSeason\"]),\n",
        "        \"Season_CruiseSeason\":  (ctx_cruise,   SEASON_DELTAS[\"Season_CruiseSeason\"]),\n",
        "        \"Season_Snowbird\":      (ctx_snowbird, SEASON_DELTAS[\"Season_Snowbird\"]),\n",
        "    }.items():\n",
        "        season_delta = season_delta.add(season_component(flag, mask, base), fill_value=0.0)\n",
        "\n",
        "    # University & Conventions\n",
        "    uni_flag = \"Season_University(HomeGames/Graduations/TermWeekdays)\"\n",
        "    if f\"ORIG_{uni_flag}\" in work.columns or f\"DEST_{uni_flag}\" in work.columns:\n",
        "        season_delta = season_delta.add(season_component(uni_flag, ctx_uni, -6.0), fill_value=0.0)\n",
        "\n",
        "    conv_flag = \"Season_Conventions/Legislative/QuarterEnd\"\n",
        "    if f\"ORIG_{conv_flag}\" in work.columns or f\"DEST_{conv_flag}\" in work.columns:\n",
        "        season_delta = season_delta.add(season_component(conv_flag, ctx_quarter, +5.0), fill_value=0.0)\n",
        "\n",
        "    # --- Confidence-scaled total adjustment ---\n",
        "    s_airport = (work[\"ORIG_CONF\"] + work[\"DEST_CONF\"]) / 200.0\n",
        "    total_adj = (market_delta + lex_delta + season_delta) * s_airport.astype(\"float32\")\n",
        "\n",
        "    # --- Final TPI, label, confidence ---\n",
        "    tpi = (baseline + total_adj).clip(0.0, 100.0)\n",
        "    low, high = THRESHOLDS\n",
        "    label = np.where(tpi >= high, \"Business-heavy\",\n",
        "                     np.where(tpi <= low, \"Leisure-heavy\", \"Mixed\"))\n",
        "    b = np.sqrt((work[\"ORIG_CONF\"]/100.0) * (work[\"DEST_CONF\"]/100.0))\n",
        "    half_gap = (high - low) / 2.0\n",
        "    center = (high + low) / 2.0\n",
        "    mgn = np.where(label==\"Business-heavy\", np.minimum(1.0, (tpi - high)/half_gap),\n",
        "          np.where(label==\"Leisure-heavy\", np.minimum(1.0, (low - tpi)/half_gap),\n",
        "                   1.0 - np.minimum(1.0, np.abs(tpi - center)/half_gap)))\n",
        "    w_air, w_m = CONF_WEIGHTS\n",
        "    conf = np.clip(100.0 * (w_air * b + w_m * mgn), 0.0, 100.0)\n",
        "\n",
        "    # --- Prepare outputs in 't' and assign back safely ---\n",
        "    t[\"TPI\"] = pd.Series(np.nan, index=t.index, dtype=\"float32\")\n",
        "    t[\"TPI_Label\"] = pd.Series(pd.NA, index=t.index, dtype=\"object\")\n",
        "    t[\"TPI_Confidence\"] = pd.Series(np.nan, index=t.index, dtype=\"float32\")\n",
        "\n",
        "    work[\"TPI\"] = tpi.astype(\"float32\")\n",
        "    work[\"TPI_Label\"] = pd.Series(label, index=work.index, dtype=\"object\")\n",
        "    work[\"TPI_Confidence\"] = conf.astype(\"float32\")\n",
        "\n",
        "    t.loc[work.index, \"TPI\"] = work[\"TPI\"].to_numpy()\n",
        "    t.loc[work.index, \"TPI_Label\"] = work[\"TPI_Label\"].astype(\"object\").to_numpy()\n",
        "    t.loc[work.index, \"TPI_Confidence\"] = work[\"TPI_Confidence\"].to_numpy()\n",
        "\n",
        "    # --- Missing diagnostics (ALIGN MASKS TO missing.index) ---\n",
        "    missing_mask = ~can_assign\n",
        "    missing = t.loc[missing_mask].copy()\n",
        "\n",
        "    om_miss = (~orig_matched &  dest_matched).loc[missing.index].to_numpy()\n",
        "    dm_miss = ( orig_matched & ~dest_matched).loc[missing.index].to_numpy()\n",
        "    bm_miss = (~orig_matched & ~dest_matched).loc[missing.index].to_numpy()\n",
        "\n",
        "    missing[\"MISSING_REASON\"] = np.select(\n",
        "        [om_miss, dm_miss, bm_miss],\n",
        "        [\"Origin not in classification (CITY_STATE slug)\",\n",
        "         \"Destination not in classification (CITY_STATE slug)\",\n",
        "         \"Both origin and destination not in classification (CITY_STATE slug)\"],\n",
        "        default=\"Unknown\"\n",
        "    )\n",
        "\n",
        "    # --- Return both dataframes ---\n",
        "    good = t.loc[~missing_mask].copy()\n",
        "    cols = list(t100.columns) + [\"TPI\",\"TPI_Label\",\"TPI_Confidence\"]\n",
        "    missing_cols = [c for c in list(t100.columns) + [\"MISSING_REASON\",\"ORIG_CITYSTATE_SLUG\",\"DEST_CITYSTATE_SLUG\",\"TPI\",\"TPI_Label\",\"TPI_Confidence\"] if c in missing.columns]\n",
        "    return good[cols], missing[missing_cols]\n",
        "\n",
        "\n",
        "def _count_csv_rows(path: Path) -> int:\n",
        "    with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        total = -1\n",
        "        for total, _ in enumerate(f, start=0):\n",
        "            pass\n",
        "    return max(0, total)\n",
        "\n",
        "def main():\n",
        "    # Validate required columns (state columns no longer required for join)\n",
        "    hdr = pd.read_csv(T100_PATH, nrows=0)\n",
        "    required = {\"ORIGIN_CITY_NAME\",\"DEST_CITY_NAME\",\"MONTH\"}\n",
        "    missing_req = required - set(hdr.columns)\n",
        "    if missing_req:\n",
        "        raise ValueError(f\"T100 file is missing required columns: {sorted(missing_req)}\")\n",
        "\n",
        "    start = time.time()\n",
        "    class_df, season_cols, market_cols = load_classification(CLASS_PATH)\n",
        "\n",
        "    # Fresh outputs\n",
        "    for p in [OUT_PATH, MISSING_OUT, UNMATCHED_ORIG_COUNTS, UNMATCHED_DEST_COUNTS]:\n",
        "        if p.exists(): p.unlink()\n",
        "\n",
        "    total_rows = _count_csv_rows(T100_PATH)\n",
        "    print(f\"[init] Input: {T100_PATH.name}  | rows ‚âà {total_rows:,}\")\n",
        "    print(f\"[init] Classification: {CLASS_PATH.name}  | cities: {len(class_df):,}\")\n",
        "    print(f\"[init] Outputs: {OUT_PATH.name}, {MISSING_OUT.name}\")\n",
        "    print(\"-\"*80, flush=True)\n",
        "\n",
        "    processed = 0\n",
        "    batch_idx = 0\n",
        "    header_written = False\n",
        "    missing_header_written = False\n",
        "\n",
        "    # Diagnostics counters\n",
        "    unmatched_rows_total = 0\n",
        "    orig_counter = Counter()\n",
        "    dest_counter = Counter()\n",
        "\n",
        "    for chunk in pd.read_csv(T100_PATH, chunksize=CHUNK_SIZE):\n",
        "        batch_idx += 1\n",
        "        t0 = time.time()\n",
        "\n",
        "        good, missing_df = compute_tpi_city_state(chunk, class_df, season_cols, market_cols)\n",
        "\n",
        "        # Append good rows\n",
        "        good.to_csv(OUT_PATH, index=False, mode=\"a\", header=not header_written)\n",
        "        header_written = True\n",
        "\n",
        "        # Append missing rows + update diagnostics\n",
        "        if not missing_df.empty:\n",
        "            missing_df.to_csv(MISSING_OUT, index=False, mode=\"a\", header=not missing_header_written)\n",
        "            missing_header_written = True\n",
        "\n",
        "            unmatched_rows_total += len(missing_df)\n",
        "\n",
        "            # Update city/state counters from the raw T100 names\n",
        "            if \"ORIGIN_CITY_NAME\" in missing_df.columns:\n",
        "                for name in missing_df[\"ORIGIN_CITY_NAME\"]:\n",
        "                    if pd.notna(name):\n",
        "                        # derive (City, ST) pair by simple split on last comma for diagnostics\n",
        "                        parts = str(name).rsplit(\",\", 1)\n",
        "                        city = parts[0].strip()\n",
        "                        st = parts[1].strip() if len(parts) == 2 else \"\"\n",
        "                        orig_counter[(city, st)] += 1\n",
        "            if \"DEST_CITY_NAME\" in missing_df.columns:\n",
        "                for name in missing_df[\"DEST_CITY_NAME\"]:\n",
        "                    if pd.notna(name):\n",
        "                        parts = str(name).rsplit(\",\", 1)\n",
        "                        city = parts[0].strip()\n",
        "                        st = parts[1].strip() if len(parts) == 2 else \"\"\n",
        "                        dest_counter[(city, st)] += 1\n",
        "\n",
        "        processed += len(chunk)\n",
        "        elapsed = time.time() - t0\n",
        "        pct = (processed / total_rows * 100.0) if total_rows else 0.0\n",
        "        out_mb = OUT_PATH.stat().st_size / (1024*1024)\n",
        "        miss_mb = (MISSING_OUT.stat().st_size / (1024*1024)) if MISSING_OUT.exists() else 0.0\n",
        "        print(f\"[batch {batch_idx:02d}] {len(chunk):,} rows in {elapsed:0.2f}s | \"\n",
        "              f\"total {processed:,}/{total_rows:,} ({pct:0.1f}%) | \"\n",
        "              f\"out: {OUT_PATH.name} ({out_mb:0.2f} MB) | missing: {miss_mb:0.2f} MB\", flush=True)\n",
        "\n",
        "    # Write unmatched city/state counts\n",
        "    if orig_counter:\n",
        "        df_orig = (pd.DataFrame([(c, s, n) for (c, s), n in orig_counter.items()],\n",
        "                                columns=[\"Origin_City\",\"Origin_State\",\"Unmatched_Row_Count\"])\n",
        "                   .sort_values([\"Unmatched_Row_Count\",\"Origin_State\",\"Origin_City\"], ascending=[False, True, True]))\n",
        "        df_orig.to_csv(UNMATCHED_ORIG_COUNTS, index=False)\n",
        "    else:\n",
        "        pd.DataFrame(columns=[\"Origin_City\",\"Origin_State\",\"Unmatched_Row_Count\"]).to_csv(UNMATCHED_ORIG_COUNTS, index=False)\n",
        "\n",
        "    if dest_counter:\n",
        "        df_dest = (pd.DataFrame([(c, s, n) for (c, s), n in dest_counter.items()],\n",
        "                                columns=[\"Dest_City\",\"Dest_State\",\"Unmatched_Row_Count\"])\n",
        "                   .sort_values([\"Unmatched_Row_Count\",\"Dest_State\",\"Dest_City\"], ascending=[False, True, True]))\n",
        "        df_dest.to_csv(UNMATCHED_DEST_COUNTS, index=False)\n",
        "    else:\n",
        "        pd.DataFrame(columns=[\"Dest_City\",\"Dest_State\",\"Unmatched_Row_Count\"]).to_csv(UNMATCHED_DEST_COUNTS, index=False)\n",
        "\n",
        "    total_elapsed = time.time() - start\n",
        "    out_mb = OUT_PATH.stat().st_size / (1024*1024)\n",
        "    miss_mb = (MISSING_OUT.stat().st_size / (1024*1024)) if MISSING_OUT.exists() else 0.0\n",
        "    print(\"-\"*80)\n",
        "    print(f\"[done] Wrote GOOD rows: {OUT_PATH}  ({out_mb:0.2f} MB)\")\n",
        "    print(f\"[done] Wrote MISSING rows: {MISSING_OUT}  ({miss_mb:0.2f} MB)\")\n",
        "    print(f\"[done] Total input rows: {processed:,}  | batches: {batch_idx}\")\n",
        "    print(f\"[done] Unmatched rows (cannot assign TPI via CITY_STATE slug): {unmatched_rows_total:,}\")\n",
        "    print(f\"[done] Unique unmatched Origin city/state: {len(orig_counter):,}  | saved -> {UNMATCHED_ORIG_COUNTS.name}\")\n",
        "    print(f\"[done] Unique unmatched Dest city/state:   {len(dest_counter):,}  | saved -> {UNMATCHED_DEST_COUNTS.name}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cvpuF9w775j",
        "outputId": "c21ecf12-c4ae-4f3d-f95b-de9faec3df0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[init] Input: consolidated_t100_data_cleaned.csv  | rows ‚âà 815,641\n",
            "[init] Classification: city_business_leisure_classification_detailed.csv  | cities: 1,629\n",
            "[init] Outputs: consolidated_t100_with_tpi_city_state.csv, missing_consolidated_t100_with_tpi_city_state.csv\n",
            "--------------------------------------------------------------------------------\n",
            "[batch 01] 250,000 rows in 12.49s | total 250,000/815,641 (30.7%) | out: consolidated_t100_with_tpi_city_state.csv (70.68 MB) | missing: 0.00 MB\n",
            "[batch 02] 250,000 rows in 11.81s | total 500,000/815,641 (61.3%) | out: consolidated_t100_with_tpi_city_state.csv (141.52 MB) | missing: 0.00 MB\n",
            "[batch 03] 250,000 rows in 10.56s | total 750,000/815,641 (92.0%) | out: consolidated_t100_with_tpi_city_state.csv (211.96 MB) | missing: 0.00 MB\n",
            "[batch 04] 65,641 rows in 3.77s | total 815,641/815,641 (100.0%) | out: consolidated_t100_with_tpi_city_state.csv (230.37 MB) | missing: 0.00 MB\n",
            "--------------------------------------------------------------------------------\n",
            "[done] Wrote GOOD rows: /content/drive/MyDrive/airline_data_analysis_v2/consolidated_t100_with_tpi_city_state.csv  (230.37 MB)\n",
            "[done] Wrote MISSING rows: /content/drive/MyDrive/airline_data_analysis_v2/missing_consolidated_t100_with_tpi_city_state.csv  (0.00 MB)\n",
            "[done] Total input rows: 815,641  | batches: 4\n",
            "[done] Unmatched rows (cannot assign TPI via CITY_STATE slug): 8\n",
            "[done] Unique unmatched Origin city/state: 4  | saved -> unmatched_origin_city_state_counts.csv\n",
            "[done] Unique unmatched Dest city/state:   4  | saved -> unmatched_dest_city_state_counts.csv\n"
          ]
        }
      ]
    }
  ]
}