{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WGgCuPeafd8g"
      ],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPSqCeREGpcrwfX5orWFjdM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrenikraxit/T-100-Analysis/blob/main/Hypothesis_3_The_Great_Hub_Hierarchy_Inversion_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis_3_The_Great_Hub_Hierarchy_Weakening"
      ],
      "metadata": {
        "id": "WGgCuPeafd8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oq3E1El666e",
        "outputId": "c1d3c1bd-9c6c-46d4-c9e6-6ad6e618ae9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 0 - Environment preparation\n",
        "\n",
        "# Connect with google drive (for Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Hypothesis 3: Hub Hierarchy Inversion Analysis\n",
        "Testing: Traditional airport hierarchy demonstrates systematic inversion with smaller airports outperforming larger ones\n",
        "\n",
        "This script performs comprehensive statistical analysis to examine whether the traditional\n",
        "airport hierarchy is inverting, with smaller airports demonstrating superior growth performance\n",
        "compared to larger airports in the post-COVID recovery period.\n",
        "\n",
        "Statistical Analysis Framework:\n",
        "- Inverse correlation analysis between airport size and growth performance\n",
        "- Hierarchical performance matrix verification\n",
        "- Market concentration analysis using Herfindahl-Hirschman Index\n",
        "- Statistical significance testing for hierarchy inversion patterns\n",
        "\n",
        "Author: Aviation Recovery Research\n",
        "Date: 2025\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def main():\n",
        "    print(\"=== HYPOTHESIS 3: HUB HIERARCHY INVERSION ANALYSIS ===\")\n",
        "    print(\"Testing: Traditional airport hierarchy demonstrates systematic inversion with smaller airports outperforming larger ones\")\n",
        "    print()\n",
        "\n",
        "    # Data Loading from Previous Analysis\n",
        "    print(\"Data Loading and Initial Processing...\")\n",
        "\n",
        "    try:\n",
        "        # Load the airport analysis from Hypothesis 1\n",
        "        airport_data = pd.read_csv('/content/drive/MyDrive/airline_data_analysis_v2/hyp1_outputs/hypothesis1_updated_airport_analysis.csv')\n",
        "        print(f\"Loaded airport analysis data: {len(airport_data):,} airports\")\n",
        "\n",
        "        # Load growth by hub results\n",
        "        hub_growth = pd.read_csv('/content/drive/MyDrive/airline_data_analysis_v2/hyp1_outputs/hypothesis1_updated_growth_by_hub.csv', index_col=0)\n",
        "        print(f\"Loaded hub growth data: {len(hub_growth)} hub types\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Hypothesis 1 results not found. Please run Hypothesis 1 analysis first.\")\n",
        "        return None\n",
        "\n",
        "    # Prepare data for hierarchy analysis\n",
        "    meaningful_airports = airport_data[\n",
        "        (airport_data['PASSENGERS_PRE'] >= 10000) &\n",
        "        (airport_data['PASSENGERS_POST'] >= 10000) &\n",
        "        (airport_data['HUB_TYPE'].notna())\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"Airports included in hierarchy analysis: {len(meaningful_airports):,}\")\n",
        "    print(f\"Hub types represented: {meaningful_airports['HUB_TYPE'].unique()}\")\n",
        "    print()\n",
        "\n",
        "    # Metric 1: Inverse Correlation Analysis\n",
        "    print(\"METRIC 1: INVERSE CORRELATION ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    def perform_correlation_analysis():\n",
        "        \"\"\"Analyze correlation between airport size and growth performance\"\"\"\n",
        "\n",
        "        # Remove outliers for more robust correlation analysis\n",
        "        growth_data = meaningful_airports['CHANGE_PERCENT']\n",
        "        size_data = meaningful_airports['PASSENGERS_PRE']\n",
        "\n",
        "        # Calculate percentiles for outlier removal\n",
        "        growth_q1, growth_q3 = growth_data.quantile([0.25, 0.75])\n",
        "        growth_iqr = growth_q3 - growth_q1\n",
        "        growth_lower = growth_q1 - 1.5 * growth_iqr\n",
        "        growth_upper = growth_q3 + 1.5 * growth_iqr\n",
        "\n",
        "        size_q1, size_q3 = size_data.quantile([0.25, 0.75])\n",
        "        size_iqr = size_q3 - size_q1\n",
        "        size_lower = size_q1 - 1.5 * size_iqr\n",
        "        size_upper = size_q3 + 1.5 * size_iqr\n",
        "\n",
        "        # Filter outliers\n",
        "        outlier_mask = (\n",
        "            (growth_data >= growth_lower) & (growth_data <= growth_upper) &\n",
        "            (size_data >= size_lower) & (size_data <= size_upper)\n",
        "        )\n",
        "\n",
        "        filtered_airports = meaningful_airports[outlier_mask].copy()\n",
        "\n",
        "        print(f\"Original sample size: {len(meaningful_airports)}\")\n",
        "        print(f\"Sample size after outlier removal: {len(filtered_airports)}\")\n",
        "        print(f\"Outliers removed: {len(meaningful_airports) - len(filtered_airports)} ({((len(meaningful_airports) - len(filtered_airports)) / len(meaningful_airports) * 100):.1f}%)\")\n",
        "        print()\n",
        "\n",
        "        # Perform correlation analyses\n",
        "        if len(filtered_airports) >= 10:\n",
        "            # Pearson correlation (linear relationship)\n",
        "            pearson_corr, pearson_p = pearsonr(filtered_airports['PASSENGERS_PRE'],\n",
        "                                              filtered_airports['CHANGE_PERCENT'])\n",
        "\n",
        "            # Spearman correlation (monotonic relationship)\n",
        "            spearman_corr, spearman_p = spearmanr(filtered_airports['PASSENGERS_PRE'],\n",
        "                                                 filtered_airports['CHANGE_PERCENT'])\n",
        "\n",
        "            print(\"CORRELATION ANALYSIS RESULTS:\")\n",
        "            print(\"-\" * 40)\n",
        "            print(f\"Pearson correlation coefficient: {pearson_corr:.4f}\")\n",
        "            print(f\"Pearson p-value: {pearson_p:.6f}\")\n",
        "            print(f\"Spearman correlation coefficient: {spearman_corr:.4f}\")\n",
        "            print(f\"Spearman p-value: {spearman_p:.6f}\")\n",
        "            print()\n",
        "\n",
        "            # Interpret correlation strength and direction\n",
        "            def interpret_correlation(corr_coef):\n",
        "                if abs(corr_coef) < 0.1:\n",
        "                    return \"negligible\"\n",
        "                elif abs(corr_coef) < 0.3:\n",
        "                    return \"weak\"\n",
        "                elif abs(corr_coef) < 0.5:\n",
        "                    return \"moderate\"\n",
        "                else:\n",
        "                    return \"strong\"\n",
        "\n",
        "            pearson_strength = interpret_correlation(pearson_corr)\n",
        "            spearman_strength = interpret_correlation(spearman_corr)\n",
        "\n",
        "            print(\"CORRELATION INTERPRETATION:\")\n",
        "            print(f\"Pearson correlation strength: {pearson_strength}\")\n",
        "            print(f\"Spearman correlation strength: {spearman_strength}\")\n",
        "\n",
        "            if pearson_corr < 0 and pearson_p < 0.05:\n",
        "                print(\"Statistical result: Significant negative correlation - smaller airports demonstrate superior growth\")\n",
        "            elif pearson_corr > 0 and pearson_p < 0.05:\n",
        "                print(\"Statistical result: Significant positive correlation - larger airports demonstrate superior growth\")\n",
        "            else:\n",
        "                print(\"Statistical result: No significant correlation between airport size and growth\")\n",
        "\n",
        "            return {\n",
        "                'pearson_corr': pearson_corr,\n",
        "                'pearson_p': pearson_p,\n",
        "                'spearman_corr': spearman_corr,\n",
        "                'spearman_p': spearman_p,\n",
        "                'sample_size': len(filtered_airports),\n",
        "                'outliers_removed': len(meaningful_airports) - len(filtered_airports)\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            print(\"Warning: Insufficient sample size for correlation analysis\")\n",
        "            return None\n",
        "\n",
        "    correlation_results = perform_correlation_analysis()\n",
        "    print()\n",
        "\n",
        "    # Metric 2: Hierarchical Performance Matrix\n",
        "    print(\"METRIC 2: HIERARCHICAL PERFORMANCE MATRIX\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    def create_performance_matrix():\n",
        "        \"\"\"Create comprehensive performance matrix by hub type\"\"\"\n",
        "\n",
        "        # Calculate hub-level statistics\n",
        "        hub_stats = meaningful_airports.groupby('HUB_TYPE').agg({\n",
        "            'PASSENGERS_PRE': ['count', 'mean', 'sum'],\n",
        "            'PASSENGERS_POST': 'sum',\n",
        "            'CHANGE_PERCENT': ['mean', 'median', 'std']\n",
        "        }).round(2)\n",
        "\n",
        "        # Flatten column names\n",
        "        hub_stats.columns = ['Airport_Count', 'Avg_Size_Pre', 'Total_Pre', 'Total_Post',\n",
        "                            'Avg_Growth', 'Median_Growth', 'Std_Growth']\n",
        "\n",
        "        # Calculate aggregate growth rates\n",
        "        hub_stats['Aggregate_Growth_Rate'] = ((hub_stats['Total_Post'] - hub_stats['Total_Pre']) /\n",
        "                                             hub_stats['Total_Pre'] * 100).round(2)\n",
        "\n",
        "        # Calculate winner statistics\n",
        "        winner_stats = meaningful_airports.groupby('HUB_TYPE').apply(\n",
        "            lambda x: pd.Series({\n",
        "                'Winners': (x['CHANGE_PERCENT'] >= 5).sum(),\n",
        "                'Stable': ((x['CHANGE_PERCENT'] > -5) & (x['CHANGE_PERCENT'] < 5)).sum(),\n",
        "                'Losers': (x['CHANGE_PERCENT'] <= -5).sum()\n",
        "            })\n",
        "        )\n",
        "\n",
        "        # Calculate winner rates\n",
        "        winner_stats['Total_Airports'] = winner_stats.sum(axis=1)\n",
        "        winner_stats['Winner_Rate'] = (winner_stats['Winners'] / winner_stats['Total_Airports'] * 100).round(1)\n",
        "\n",
        "        # Combine results\n",
        "        performance_matrix = pd.merge(hub_stats, winner_stats[['Winner_Rate']], left_index=True, right_index=True)\n",
        "\n",
        "        # Define theoretical hierarchy (by size, largest to smallest)\n",
        "        size_hierarchy = ['Major Hub', 'Medium Hub', 'Small Hub', 'Non-Hub Primary', 'Regional/Local']\n",
        "        performance_hierarchy = performance_matrix.sort_values('Avg_Growth', ascending=False).index.tolist()\n",
        "\n",
        "        print(\"HIERARCHICAL PERFORMANCE MATRIX:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Hub Type':<15} {'Count':<6} {'Avg Size (K)':<12} {'Avg Growth':<11} {'Winner %':<9} {'Aggregate %':<11}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for hub_type in performance_hierarchy:\n",
        "            if hub_type in performance_matrix.index:\n",
        "                row = performance_matrix.loc[hub_type]\n",
        "                avg_size = row['Avg_Size_Pre'] / 1000  # Convert to thousands\n",
        "                print(f\"{hub_type:<15} {int(row['Airport_Count']):<6} {avg_size:<12,.0f} \"\n",
        "                      f\"{row['Avg_Growth']:<11.1f} {row['Winner_Rate']:<9.1f} \"\n",
        "                      f\"{row['Aggregate_Growth_Rate']:<11.1f}\")\n",
        "\n",
        "        print()\n",
        "        print(\"HIERARCHY COMPARISON:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"Traditional hierarchy (by size):\")\n",
        "        for i, hub_type in enumerate(size_hierarchy, 1):\n",
        "            if hub_type in performance_matrix.index:\n",
        "                print(f\"  {i}. {hub_type}\")\n",
        "\n",
        "        print(\"\\nPerformance hierarchy (by growth):\")\n",
        "        for i, hub_type in enumerate(performance_hierarchy, 1):\n",
        "            print(f\"  {i}. {hub_type}\")\n",
        "\n",
        "        # Check for inversion\n",
        "        hierarchy_inverted = False\n",
        "        if len(performance_hierarchy) >= 3:\n",
        "            # Check if smaller hubs are performing better than larger hubs\n",
        "            small_hub_rank = performance_hierarchy.index('Small Hub') if 'Small Hub' in performance_hierarchy else 999\n",
        "            major_hub_rank = performance_hierarchy.index('Major Hub') if 'Major Hub' in performance_hierarchy else 999\n",
        "\n",
        "            if small_hub_rank < major_hub_rank:  # Lower rank = better performance\n",
        "                hierarchy_inverted = True\n",
        "                print(f\"\\nHIERARCHY INVERSION DETECTED: Small hubs (rank {small_hub_rank + 1}) outperform major hubs (rank {major_hub_rank + 1})\")\n",
        "            else:\n",
        "                print(f\"\\nNO HIERARCHY INVERSION: Traditional hierarchy maintained\")\n",
        "\n",
        "        return performance_matrix, hierarchy_inverted\n",
        "\n",
        "    performance_matrix, hierarchy_inverted = create_performance_matrix()\n",
        "    print()\n",
        "\n",
        "    # Metric 3: Market Concentration Analysis\n",
        "    print(\"METRIC 3: MARKET CONCENTRATION ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    def calculate_market_concentration():\n",
        "        \"\"\"Calculate Herfindahl-Hirschman Index for market concentration\"\"\"\n",
        "\n",
        "        def calculate_hhi(passenger_shares):\n",
        "            \"\"\"Calculate HHI from passenger shares (as percentages)\"\"\"\n",
        "            return sum(share**2 for share in passenger_shares)\n",
        "\n",
        "        # Calculate market shares for pre-COVID and post-COVID\n",
        "        total_pre = meaningful_airports['PASSENGERS_PRE'].sum()\n",
        "        total_post = meaningful_airports['PASSENGERS_POST'].sum()\n",
        "\n",
        "        pre_shares = (meaningful_airports['PASSENGERS_PRE'] / total_pre * 100).values\n",
        "        post_shares = (meaningful_airports['PASSENGERS_POST'] / total_post * 100).values\n",
        "\n",
        "        hhi_pre = calculate_hhi(pre_shares)\n",
        "        hhi_post = calculate_hhi(post_shares)\n",
        "        hhi_change = hhi_post - hhi_pre\n",
        "\n",
        "        print(\"MARKET CONCENTRATION ANALYSIS (Herfindahl-Hirschman Index):\")\n",
        "        print(\"-\" * 65)\n",
        "        print(f\"Pre-COVID HHI: {hhi_pre:.1f}\")\n",
        "        print(f\"Post-COVID HHI: {hhi_post:.1f}\")\n",
        "        print(f\"HHI change: {hhi_change:+.1f}\")\n",
        "        print()\n",
        "\n",
        "        # Interpret HHI values\n",
        "        def interpret_hhi(hhi_value):\n",
        "            if hhi_value < 1500:\n",
        "                return \"unconcentrated\"\n",
        "            elif hhi_value < 2500:\n",
        "                return \"moderately concentrated\"\n",
        "            else:\n",
        "                return \"highly concentrated\"\n",
        "\n",
        "        pre_concentration = interpret_hhi(hhi_pre)\n",
        "        post_concentration = interpret_hhi(hhi_post)\n",
        "\n",
        "        print(\"CONCENTRATION INTERPRETATION:\")\n",
        "        print(f\"Pre-COVID market: {pre_concentration}\")\n",
        "        print(f\"Post-COVID market: {post_concentration}\")\n",
        "\n",
        "        if hhi_change < -50:\n",
        "            print(\"Market concentration change: Significant deconcentration\")\n",
        "        elif hhi_change < -10:\n",
        "            print(\"Market concentration change: Moderate deconcentration\")\n",
        "        elif hhi_change < 0:\n",
        "            print(\"Market concentration change: Slight deconcentration\")\n",
        "        elif hhi_change > 50:\n",
        "            print(\"Market concentration change: Significant concentration increase\")\n",
        "        elif hhi_change > 10:\n",
        "            print(\"Market concentration change: Moderate concentration increase\")\n",
        "        else:\n",
        "            print(\"Market concentration change: Minimal change\")\n",
        "\n",
        "        # Calculate concentration by hub type\n",
        "        hub_concentration = meaningful_airports.groupby('HUB_TYPE').agg({\n",
        "            'PASSENGERS_PRE': 'sum',\n",
        "            'PASSENGERS_POST': 'sum'\n",
        "        })\n",
        "\n",
        "        hub_concentration['Market_Share_Pre'] = (hub_concentration['PASSENGERS_PRE'] / total_pre * 100).round(2)\n",
        "        hub_concentration['Market_Share_Post'] = (hub_concentration['PASSENGERS_POST'] / total_post * 100).round(2)\n",
        "        hub_concentration['Share_Change'] = (hub_concentration['Market_Share_Post'] -\n",
        "                                           hub_concentration['Market_Share_Pre']).round(2)\n",
        "\n",
        "        print()\n",
        "        print(\"MARKET SHARE BY HUB TYPE:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"{'Hub Type':<15} {'Pre-COVID %':<11} {'Post-COVID %':<12} {'Change':<8}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for hub_type, row in hub_concentration.iterrows():\n",
        "            change_indicator = \"+\" if row['Share_Change'] >= 0 else \"\"\n",
        "            print(f\"{hub_type:<15} {row['Market_Share_Pre']:<11.2f} {row['Market_Share_Post']:<12.2f} \"\n",
        "                  f\"{change_indicator}{row['Share_Change']:<8.2f}\")\n",
        "\n",
        "        return {\n",
        "            'hhi_pre': hhi_pre,\n",
        "            'hhi_post': hhi_post,\n",
        "            'hhi_change': hhi_change,\n",
        "            'hub_concentration': hub_concentration\n",
        "        }\n",
        "\n",
        "    concentration_results = calculate_market_concentration()\n",
        "    print()\n",
        "\n",
        "    # Hypothesis Evaluation\n",
        "    print(\"HYPOTHESIS EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"HYPOTHESIS: Traditional airport hierarchy demonstrates systematic inversion with smaller airports outperforming larger ones\")\n",
        "    print()\n",
        "\n",
        "    # Success criteria assessment\n",
        "    criteria_met = 0\n",
        "\n",
        "    # Criterion 1: Negative correlation between size and growth\n",
        "    print(\"CRITERION 1: Negative Size-Growth Correlation\")\n",
        "    if correlation_results:\n",
        "        pearson_corr = correlation_results['pearson_corr']\n",
        "        pearson_p = correlation_results['pearson_p']\n",
        "\n",
        "        print(f\"  Pearson correlation: {pearson_corr:.4f} (p = {pearson_p:.6f})\")\n",
        "\n",
        "        if pearson_corr < 0 and pearson_p < 0.05:\n",
        "            print(\"  CRITERION 1 MET: Significant negative correlation between airport size and growth\")\n",
        "            criteria_met += 1\n",
        "        elif pearson_corr < 0:\n",
        "            print(\"  CRITERION 1 PARTIALLY MET: Negative correlation but not statistically significant\")\n",
        "        else:\n",
        "            print(\"  CRITERION 1 NOT MET: No negative correlation between size and growth\")\n",
        "    else:\n",
        "        print(\"  CRITERION 1 CANNOT BE EVALUATED: Correlation analysis failed\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Criterion 2: Hierarchy inversion in performance rankings\n",
        "    print(\"CRITERION 2: Hierarchy Performance Inversion\")\n",
        "\n",
        "    # Calculate growth advantage from existing data\n",
        "    if 'Small Hub' in performance_matrix.index and 'Major Hub' in performance_matrix.index:\n",
        "        small_hub_growth = performance_matrix.loc['Small Hub', 'Avg_Growth']\n",
        "        major_hub_growth = performance_matrix.loc['Major Hub', 'Avg_Growth']\n",
        "        growth_advantage = small_hub_growth - major_hub_growth\n",
        "\n",
        "        print(f\"  Small Hub average growth: {small_hub_growth:.1f}%\")\n",
        "        print(f\"  Major Hub average growth: {major_hub_growth:.1f}%\")\n",
        "        print(f\"  Small hub advantage: {growth_advantage:+.1f} percentage points\")\n",
        "\n",
        "        if hierarchy_inverted and growth_advantage > 2:\n",
        "            print(\"  CRITERION 2 MET: Clear hierarchy inversion with meaningful difference\")\n",
        "            criteria_met += 1\n",
        "        elif hierarchy_inverted:\n",
        "            print(\"  CRITERION 2 PARTIALLY MET: Hierarchy inversion but small difference\")\n",
        "        else:\n",
        "            print(\"  CRITERION 2 NOT MET: No hierarchy inversion\")\n",
        "    else:\n",
        "        print(\"  CRITERION 2 CANNOT BE EVALUATED: Insufficient hub type data\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Criterion 3: Market deconcentration\n",
        "    print(\"CRITERION 3: Decreased Market Concentration\")\n",
        "    hhi_change = concentration_results['hhi_change']\n",
        "\n",
        "    print(f\"  HHI change: {hhi_change:+.1f}\")\n",
        "\n",
        "    if hhi_change < -50:\n",
        "        print(\"  CRITERION 3 STRONGLY MET: Significant deconcentration (HHI decrease > 50)\")\n",
        "        criteria_met += 1\n",
        "    elif hhi_change < -10:\n",
        "        print(\"  CRITERION 3 MET: Meaningful deconcentration (HHI decrease > 10)\")\n",
        "        criteria_met += 1\n",
        "    elif hhi_change < 0:\n",
        "        print(\"  CRITERION 3 PARTIALLY MET: Some deconcentration but minimal\")\n",
        "    else:\n",
        "        print(\"  CRITERION 3 NOT MET: No deconcentration\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Final Verdict\n",
        "    print(\"FINAL VERDICT:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Criteria met: {criteria_met}/3\")\n",
        "\n",
        "    if criteria_met == 3:\n",
        "        print(\"HYPOTHESIS STRONGLY SUPPORTED: Strong evidence for hub hierarchy inversion\")\n",
        "        print(\"The traditional airport hierarchy demonstrates clear systematic inversion\")\n",
        "    elif criteria_met == 2:\n",
        "        print(\"HYPOTHESIS SUPPORTED: Good evidence for hub hierarchy inversion\")\n",
        "        print(\"Significant evidence that smaller airports outperform larger ones\")\n",
        "    elif criteria_met == 1:\n",
        "        print(\"HYPOTHESIS PARTIALLY SUPPORTED: Limited evidence for hierarchy inversion\")\n",
        "        print(\"Some indicators support the hypothesis but evidence is mixed\")\n",
        "    else:\n",
        "        print(\"HYPOTHESIS NOT SUPPORTED: Insufficient evidence for hierarchy inversion\")\n",
        "        print(\"The traditional hierarchy appears to be maintained\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Additional Analysis: Detailed Hub Performance\n",
        "    print(\"ADDITIONAL INSIGHTS: DETAILED HUB PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Top and bottom performers across all hub types\n",
        "    print(\"OVERALL TOP AND BOTTOM PERFORMERS:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    top_performers = meaningful_airports.nlargest(10, 'CHANGE_PERCENT')\n",
        "    bottom_performers = meaningful_airports.nsmallest(10, 'CHANGE_PERCENT')\n",
        "\n",
        "    print(\"Top 10 Growth Performers:\")\n",
        "    for i, (_, airport) in enumerate(top_performers.iterrows(), 1):\n",
        "        hub_type = airport['HUB_TYPE']\n",
        "        airport_name = airport.get('AIRPORT_NAME', 'Unknown')\n",
        "        print(f\"{i:2d}. {airport['AIRPORT']} ({hub_type}): {airport['CHANGE_PERCENT']:+.1f}%\")\n",
        "\n",
        "    print(\"\\nBottom 10 Performers:\")\n",
        "    for i, (_, airport) in enumerate(bottom_performers.iterrows(), 1):\n",
        "        hub_type = airport['HUB_TYPE']\n",
        "        airport_name = airport.get('AIRPORT_NAME', 'Unknown')\n",
        "        print(f\"{i:2d}. {airport['AIRPORT']} ({hub_type}): {airport['CHANGE_PERCENT']:+.1f}%\")\n",
        "\n",
        "    # Hub type distribution in top performers\n",
        "    print(f\"\\nHUB TYPE DISTRIBUTION IN TOP 25% PERFORMERS:\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    top_quartile = meaningful_airports.nlargest(int(len(meaningful_airports) * 0.25), 'CHANGE_PERCENT')\n",
        "    hub_distribution = top_quartile['HUB_TYPE'].value_counts()\n",
        "\n",
        "    for hub_type, count in hub_distribution.items():\n",
        "        total_of_type = len(meaningful_airports[meaningful_airports['HUB_TYPE'] == hub_type])\n",
        "        percentage = (count / total_of_type) * 100\n",
        "        print(f\"{hub_type}: {count}/{total_of_type} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Save results for further analysis\n",
        "    try:\n",
        "        correlation_df = pd.DataFrame([correlation_results]) if correlation_results else pd.DataFrame()\n",
        "        correlation_df.to_csv('/content/drive/MyDrive/airline_data_analysis_v2/hyp3_outputs/hypothesis3_correlation_analysis.csv', index=False)\n",
        "\n",
        "        performance_matrix.to_csv('/content/drive/MyDrive/airline_data_analysis_v2/hyp3_outputs/hypothesis3_performance_matrix.csv')\n",
        "\n",
        "        concentration_df = pd.DataFrame([concentration_results])\n",
        "        concentration_df.to_csv('/content/drive/MyDrive/airline_data_analysis_v2/hyp3_outputs/hypothesis3_concentration_analysis.csv', index=False)\n",
        "\n",
        "        # Create summary\n",
        "        summary_results = pd.DataFrame({\n",
        "            'Criterion': ['Size-Growth Correlation', 'Hierarchy Inversion', 'Market Deconcentration'],\n",
        "            'Status': [\n",
        "                'MET' if correlation_results and correlation_results['pearson_corr'] < 0 and correlation_results['pearson_p'] < 0.05 else 'NOT MET',\n",
        "                'MET' if hierarchy_inverted else 'NOT MET',\n",
        "                'MET' if hhi_change < -10 else 'NOT MET'\n",
        "            ],\n",
        "            'Value': [\n",
        "                f\"r = {correlation_results['pearson_corr']:.3f} (p = {correlation_results['pearson_p']:.3f})\" if correlation_results else \"N/A\",\n",
        "                f\"Small Hub rank: 1st\" if hierarchy_inverted else \"No inversion\",\n",
        "                f\"HHI change: {hhi_change:+.1f}\"\n",
        "            ],\n",
        "            'Interpretation': [\n",
        "                'Negative correlation between size and growth',\n",
        "                'Small hubs outperform larger hubs',\n",
        "                'Market became less concentrated'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "        summary_results.to_csv('/content/drive/MyDrive/airline_data_analysis_v2/hyp3_outputs/hypothesis3_summary_results.csv', index=False)\n",
        "\n",
        "        print(\"\\nResults saved for subsequent analysis\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nNote: Results not saved ({e})\")\n",
        "\n",
        "    print()\n",
        "    print(\"=== ANALYSIS COMPLETE ===\")\n",
        "    print(\"Review the correlation analysis, hierarchy comparison, and concentration metrics to assess hypothesis validity.\")\n",
        "\n",
        "    return {\n",
        "        'correlation_results': correlation_results,\n",
        "        'performance_matrix': performance_matrix,\n",
        "        'concentration_results': concentration_results,\n",
        "        'hierarchy_inverted': hierarchy_inverted,\n",
        "        'criteria_met': criteria_met\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHw6MsR-k15b",
        "outputId": "8684b7fb-31b4-44db-9568-ba9c7b827730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== HYPOTHESIS 3: HUB HIERARCHY INVERSION ANALYSIS ===\n",
            "Testing: Traditional airport hierarchy demonstrates systematic inversion with smaller airports outperforming larger ones\n",
            "\n",
            "Data Loading and Initial Processing...\n",
            "Loaded airport analysis data: 418 airports\n",
            "Loaded hub growth data: 4 hub types\n",
            "Airports included in hierarchy analysis: 418\n",
            "Hub types represented: ['Non-Hub Primary' 'Medium Hub' 'Small Hub' 'Major Hub']\n",
            "\n",
            "METRIC 1: INVERSE CORRELATION ANALYSIS\n",
            "============================================================\n",
            "Original sample size: 418\n",
            "Sample size after outlier removal: 328\n",
            "Outliers removed: 90 (21.5%)\n",
            "\n",
            "CORRELATION ANALYSIS RESULTS:\n",
            "----------------------------------------\n",
            "Pearson correlation coefficient: 0.2690\n",
            "Pearson p-value: 0.000001\n",
            "Spearman correlation coefficient: 0.2450\n",
            "Spearman p-value: 0.000007\n",
            "\n",
            "CORRELATION INTERPRETATION:\n",
            "Pearson correlation strength: weak\n",
            "Spearman correlation strength: weak\n",
            "Statistical result: Significant positive correlation - larger airports demonstrate superior growth\n",
            "\n",
            "METRIC 2: HIERARCHICAL PERFORMANCE MATRIX\n",
            "============================================================\n",
            "HIERARCHICAL PERFORMANCE MATRIX:\n",
            "--------------------------------------------------------------------------------\n",
            "Hub Type        Count  Avg Size (K) Avg Growth  Winner %  Aggregate %\n",
            "--------------------------------------------------------------------------------\n",
            "Small Hub       71     1,799        18.2        69.0      17.5       \n",
            "Non-Hub Primary 278    187          7.2         37.1      4.4        \n",
            "Medium Hub      40     8,499        6.9         47.5      6.6        \n",
            "Major Hub       29     35,530       3.2         44.8      2.7        \n",
            "\n",
            "HIERARCHY COMPARISON:\n",
            "----------------------------------------\n",
            "Traditional hierarchy (by size):\n",
            "  1. Major Hub\n",
            "  2. Medium Hub\n",
            "  3. Small Hub\n",
            "  4. Non-Hub Primary\n",
            "\n",
            "Performance hierarchy (by growth):\n",
            "  1. Small Hub\n",
            "  2. Non-Hub Primary\n",
            "  3. Medium Hub\n",
            "  4. Major Hub\n",
            "\n",
            "HIERARCHY INVERSION DETECTED: Small hubs (rank 1) outperform major hubs (rank 4)\n",
            "\n",
            "METRIC 3: MARKET CONCENTRATION ANALYSIS\n",
            "============================================================\n",
            "MARKET CONCENTRATION ANALYSIS (Herfindahl-Hirschman Index):\n",
            "-----------------------------------------------------------------\n",
            "Pre-COVID HHI: 200.2\n",
            "Post-COVID HHI: 194.3\n",
            "HHI change: -5.9\n",
            "\n",
            "CONCENTRATION INTERPRETATION:\n",
            "Pre-COVID market: unconcentrated\n",
            "Post-COVID market: unconcentrated\n",
            "Market concentration change: Slight deconcentration\n",
            "\n",
            "MARKET SHARE BY HUB TYPE:\n",
            "--------------------------------------------------\n",
            "Hub Type        Pre-COVID % Post-COVID % Change  \n",
            "--------------------------------------------------\n",
            "Major Hub       66.47       65.13        -1.34   \n",
            "Medium Hub      21.93       22.29        +0.36    \n",
            "Non-Hub Primary 3.36        3.35         -0.01   \n",
            "Small Hub       8.24        9.23         +0.99    \n",
            "\n",
            "HYPOTHESIS EVALUATION\n",
            "============================================================\n",
            "HYPOTHESIS: Traditional airport hierarchy demonstrates systematic inversion with smaller airports outperforming larger ones\n",
            "\n",
            "CRITERION 1: Negative Size-Growth Correlation\n",
            "  Pearson correlation: 0.2690 (p = 0.000001)\n",
            "  CRITERION 1 NOT MET: No negative correlation between size and growth\n",
            "\n",
            "CRITERION 2: Hierarchy Performance Inversion\n",
            "  Small Hub average growth: 18.2%\n",
            "  Major Hub average growth: 3.2%\n",
            "  Small hub advantage: +15.0 percentage points\n",
            "  CRITERION 2 MET: Clear hierarchy inversion with meaningful difference\n",
            "\n",
            "CRITERION 3: Decreased Market Concentration\n",
            "  HHI change: -5.9\n",
            "  CRITERION 3 PARTIALLY MET: Some deconcentration but minimal\n",
            "\n",
            "FINAL VERDICT:\n",
            "========================================\n",
            "Criteria met: 1/3\n",
            "HYPOTHESIS PARTIALLY SUPPORTED: Limited evidence for hierarchy inversion\n",
            "Some indicators support the hypothesis but evidence is mixed\n",
            "\n",
            "ADDITIONAL INSIGHTS: DETAILED HUB PERFORMANCE ANALYSIS\n",
            "============================================================\n",
            "OVERALL TOP AND BOTTOM PERFORMERS:\n",
            "----------------------------------------\n",
            "Top 10 Growth Performers:\n",
            " 1. HVN (Non-Hub Primary): +1112.4%\n",
            " 2. VRB (Non-Hub Primary): +658.7%\n",
            " 3. PVU (Non-Hub Primary): +350.3%\n",
            " 4. JST (Non-Hub Primary): +202.8%\n",
            " 5. SRQ (Small Hub): +166.9%\n",
            " 6. CVN (Non-Hub Primary): +155.0%\n",
            " 7. SHR (Non-Hub Primary): +152.3%\n",
            " 8. BED (Non-Hub Primary): +121.0%\n",
            " 9. CYS (Non-Hub Primary): +111.7%\n",
            "10. HDN (Non-Hub Primary): +110.6%\n",
            "\n",
            "Bottom 10 Performers:\n",
            " 1. OGS (Non-Hub Primary): -76.0%\n",
            " 2. MKK (Non-Hub Primary): -73.5%\n",
            " 3. DBQ (Non-Hub Primary): -70.6%\n",
            " 4. PHF (Non-Hub Primary): -66.0%\n",
            " 5. JLN (Non-Hub Primary): -62.9%\n",
            " 6. BLD (Non-Hub Primary): -59.9%\n",
            " 7. LSE (Non-Hub Primary): -57.5%\n",
            " 8. OGD (Non-Hub Primary): -54.5%\n",
            " 9. SWF (Non-Hub Primary): -54.3%\n",
            "10. IAG (Non-Hub Primary): -53.7%\n",
            "\n",
            "HUB TYPE DISTRIBUTION IN TOP 25% PERFORMERS:\n",
            "---------------------------------------------\n",
            "Non-Hub Primary: 64/278 (23.0%)\n",
            "Small Hub: 30/71 (42.3%)\n",
            "Medium Hub: 7/40 (17.5%)\n",
            "Major Hub: 3/29 (10.3%)\n",
            "\n",
            "Results saved for subsequent analysis\n",
            "\n",
            "=== ANALYSIS COMPLETE ===\n",
            "Review the correlation analysis, hierarchy comparison, and concentration metrics to assess hypothesis validity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some more statistical testing\n",
        "- Paired t-tests for before/after centrality measures\n",
        "- Network-level statistical tests\n",
        "- Permutation tests for network metrics\n",
        "- Structural equation modeling for causal pathways"
      ],
      "metadata": {
        "id": "p7STqbBNmIuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---- Paths ----\n",
        "BASE = \"/content/drive/MyDrive/airline_data_analysis_v2\"\n",
        "IN_T100 = os.path.join(BASE, \"consolidated_t100_with_tpi_city_state.csv\")\n",
        "IN_AIRPORTS = os.path.join(BASE, \"unique_airports_t100_hub_updated.csv\")\n",
        "OUT_DIR = os.path.join(BASE, \"hyp3_outputs\")\n",
        "OUT_EDGES = os.path.join(OUT_DIR, \"h3_edges_pre_post.csv\")\n",
        "\n",
        "# ---- Config ----\n",
        "PRE_YEARS  = {2018, 2019}\n",
        "POST_YEARS = {2023, 2024}\n",
        "DOMESTIC_ONLY = True\n",
        "\n",
        "# ---- Load ----\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "df = pd.read_csv(IN_T100, low_memory=False)\n",
        "ap = pd.read_csv(IN_AIRPORTS)\n",
        "\n",
        "# ---- Clean and restrict ----\n",
        "df[\"YEAR\"] = pd.to_numeric(df[\"YEAR\"], errors=\"coerce\")\n",
        "df[\"PASSENGERS\"] = pd.to_numeric(df[\"PASSENGERS\"], errors=\"coerce\")\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"YEAR\",\"PASSENGERS\",\"ORIGIN\",\"DEST\"])\n",
        "\n",
        "if DOMESTIC_ONLY:\n",
        "    df = df[(df[\"ORIGIN_COUNTRY\"].astype(str).str.upper().str.contains(\"US\")) &\n",
        "            (df[\"DEST_COUNTRY\"].astype(str).str.upper().str.contains(\"US\"))]\n",
        "\n",
        "# ---- Build PRE / POST aggregates ----\n",
        "pre = (df[df[\"YEAR\"].isin(PRE_YEARS)]\n",
        "       .groupby([\"ORIGIN\",\"DEST\"], as_index=False)[\"PASSENGERS\"].sum()\n",
        "       .rename(columns={\"PASSENGERS\":\"W_PRE\"}))\n",
        "\n",
        "post = (df[df[\"YEAR\"].isin(POST_YEARS)]\n",
        "        .groupby([\"ORIGIN\",\"DEST\"], as_index=False)[\"PASSENGERS\"].sum()\n",
        "        .rename(columns={\"PASSENGERS\":\"W_POST\"}))\n",
        "\n",
        "# ---- Merge into single edge list ----\n",
        "edges = pd.merge(pre, post, on=[\"ORIGIN\",\"DEST\"], how=\"outer\")\n",
        "edges[\"W_PRE\"]  = edges[\"W_PRE\"].fillna(0).astype(float)\n",
        "edges[\"W_POST\"] = edges[\"W_POST\"].fillna(0).astype(float)\n",
        "edges = edges[edges[\"ORIGIN\"] != edges[\"DEST\"]].copy()\n",
        "\n",
        "# ---- Growth calculations ----\n",
        "edges[\"GROWTH_ABS\"] = edges[\"W_POST\"] - edges[\"W_PRE\"]\n",
        "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "    edges[\"GROWTH_PCT\"] = np.where(\n",
        "        edges[\"W_PRE\"] > 0,\n",
        "        100.0 * edges[\"GROWTH_ABS\"] / edges[\"W_PRE\"],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "# ---- Enrich with hub classifications ----\n",
        "ap_hubs = ap[[\"AIRPORT_NAME\",\"HUB_TYPE\"]].rename(columns={\"AIRPORT_NAME\":\"ORIGIN\",\"HUB_TYPE\":\"ORIGIN_HUB_TYPE\"})\n",
        "edges = edges.merge(ap_hubs, on=\"ORIGIN\", how=\"left\")\n",
        "\n",
        "ap_hubs = ap[[\"AIRPORT_NAME\",\"HUB_TYPE\"]].rename(columns={\"AIRPORT_NAME\":\"DEST\",\"HUB_TYPE\":\"DEST_HUB_TYPE\"})\n",
        "edges = edges.merge(ap_hubs, on=\"DEST\", how=\"left\")\n",
        "\n",
        "# ---- Save ----\n",
        "edges = edges.sort_values([\"ORIGIN\",\"DEST\"]).reset_index(drop=True)\n",
        "edges.to_csv(OUT_EDGES, index=False)\n",
        "print(f\"Saved: {OUT_EDGES}  (rows={len(edges)})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FVwPQVv38XG",
        "outputId": "4115a6c6-fae8-4365-c148-81c737a98f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/airline_data_analysis_v2/hyp3_outputs/h3_edges_pre_post.csv  (rows=40010)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "H3 â€” Network Analysis & Report Generator (GPU/RAM aware + renumbered cuGraph)\n",
        "Outputs:\n",
        "  /content/drive/MyDrive/airline_data_analysis_v2/hyp3_outputs/hyp3_stats_report.md\n",
        "  /content/drive/MyDrive/airline_data_analysis_v2/hyp3_outputs/hyp3_summary.json\n",
        "\"\"\"\n",
        "\n",
        "import os, json, time, math, sys, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from scipy import stats\n",
        "from typing import Callable, Dict, List\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "ROOT_DIR    = \"/content/drive/MyDrive/airline_data_analysis_v2\"\n",
        "INPUT_EDGES = os.path.join(ROOT_DIR, \"hyp3_outputs\", \"h3_edges_pre_post.csv\")\n",
        "OUTPUT_DIR  = os.path.join(ROOT_DIR, \"hyp3_outputs\")\n",
        "\n",
        "# Default sizes (auto-tuned higher if GPU & high-RAM)\n",
        "N_BOOT      = 500\n",
        "N_PERM      = 1000\n",
        "K_BETWEENNESS = 64\n",
        "ALPHA       = 0.05\n",
        "PRINT_EVERY = 100\n",
        "# ----------------------------------------\n",
        "\n",
        "# ---- Environment checks (GPU & RAM) ----\n",
        "print(\"=== Environment ===\")\n",
        "try:\n",
        "    import psutil\n",
        "    ram_gb = psutil.virtual_memory().total / 1e9\n",
        "    print(f\"RAM: {ram_gb:.1f} GB\")\n",
        "except Exception:\n",
        "    ram_gb = None\n",
        "    print(\"psutil not available; skipping RAM check.\")\n",
        "\n",
        "try:\n",
        "    out = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
        "    if out.returncode == 0 and \"NVIDIA-SMI\" in out.stdout:\n",
        "        print(\"GPU detected:\\n\", out.stdout.splitlines()[0])\n",
        "        HAS_GPU = True\n",
        "    else:\n",
        "        print(\"No GPU detected.\")\n",
        "        HAS_GPU = False\n",
        "except Exception:\n",
        "    print(\"No GPU detected.\")\n",
        "    HAS_GPU = False\n",
        "\n",
        "# Auto-tune sizes if strong machine\n",
        "if HAS_GPU and (ram_gb is None or ram_gb >= 20):\n",
        "    N_BOOT = max(N_BOOT, 1000)\n",
        "    N_PERM = max(N_PERM, 2000)\n",
        "    K_BETWEENNESS = max(K_BETWEENNESS, 128)\n",
        "    print(f\"Auto-tuned for GPU/high-RAM: N_BOOT={N_BOOT}, N_PERM={N_PERM}, K_BETWEENNESS={K_BETWEENNESS}\")\n",
        "else:\n",
        "    print(f\"Using conservative sizes: N_BOOT={N_BOOT}, N_PERM={N_PERM}, K_BETWEENNESS={K_BETWEENNESS}\")\n",
        "\n",
        "# Try to enable RAPIDS if GPU present\n",
        "HAS_RAPIDS = False\n",
        "if HAS_GPU:\n",
        "    try:\n",
        "        import cudf, cugraph\n",
        "        HAS_RAPIDS = True\n",
        "        print(\"RAPIDS (cuDF/cuGraph) already available.\")\n",
        "    except Exception:\n",
        "        print(\"Installing RAPIDS (cuDF/cuGraph) for CUDA 12 ...\")\n",
        "        pkgs = [\"cudf-cu12\", \"cugraph-cu12\", \"cupy-cuda12x\"]\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs, check=False)\n",
        "        try:\n",
        "            import cudf, cugraph\n",
        "            HAS_RAPIDS = True\n",
        "            print(\"RAPIDS installed and imported.\")\n",
        "        except Exception as e:\n",
        "            print(\"RAPIDS install failed; falling back to CPU.\", e)\n",
        "\n",
        "# Optional: community modularity (python-louvain). If missing, modularity will be NaN.\n",
        "try:\n",
        "    import community  # python-louvain\n",
        "    _HAS_COMMUNITY = True\n",
        "except Exception:\n",
        "    _HAS_COMMUNITY = False\n",
        "\n",
        "def ensure_dir(p):\n",
        "    if not os.path.exists(p): os.makedirs(p, exist_ok=True)\n",
        "\n",
        "# ---------------- GRAPH HELPERS (CPU) ----------------\n",
        "def build_weighted_digraph_pandas(edges: pd.DataFrame,\n",
        "                                  wcol: str,\n",
        "                                  s_col: str = \"ORIGIN\",\n",
        "                                  t_col: str = \"DEST\",\n",
        "                                  remove_self_loops: bool = True) -> nx.DiGraph:\n",
        "    G = nx.DiGraph()\n",
        "    add = G.add_edge\n",
        "    for u, v, w in edges[[s_col, t_col, wcol]].itertuples(index=False, name=None):\n",
        "        if remove_self_loops and u == v: continue\n",
        "        if G.has_edge(u, v):\n",
        "            G[u][v][\"weight\"] += float(w)\n",
        "        else:\n",
        "            add(u, v, weight=float(w))\n",
        "    return G\n",
        "\n",
        "# ---------- cuGraph builders (renumber=True) ----------\n",
        "def build_cugraph_digraph(edges: \"cudf.DataFrame\",\n",
        "                          wcol: str,\n",
        "                          s_col: str = \"ORIGIN\",\n",
        "                          t_col: str = \"DEST\",\n",
        "                          remove_self_loops: bool = True):\n",
        "    \"\"\"\n",
        "    Build a directed cuGraph.Graph with renumber=True (required when vertex cols are non-integer).\n",
        "    Returns (G, renumber_map), where renumber_map maps internal vertex ids -> original ids.\n",
        "    \"\"\"\n",
        "    import cudf, cugraph\n",
        "    df = edges[[s_col, t_col, wcol]].copy()\n",
        "    if remove_self_loops:\n",
        "        df = df[df[s_col] != df[t_col]]\n",
        "\n",
        "    # Ensure weight column name 'weight'\n",
        "    df = df.rename(columns={wcol: \"weight\"})\n",
        "\n",
        "    G = cugraph.Graph(directed=True)\n",
        "    # renumber=True lets cuGraph assign internal int ids even if ORIGIN/DEST are strings\n",
        "    G.from_cudf_edgelist(df, source=s_col, destination=t_col,\n",
        "                         edge_attr=\"weight\", renumber=True)\n",
        "    # Get renumber map (internal -> original ids). API varies slightly across versions, so be defensive.\n",
        "    renum = None\n",
        "    try:\n",
        "        renum = G.renumber_map  # Series or DataFrame depending on version\n",
        "    except Exception:\n",
        "        pass\n",
        "    return G, renum\n",
        "\n",
        "def _unrenumber_df(df, col, renum):\n",
        "    \"\"\"Try to unrenumber a DataFrame 'df' in-place given a renumber map; safe no-op if unavailable.\"\"\"\n",
        "    try:\n",
        "        import cugraph\n",
        "        from cugraph.utilities import unrenumber\n",
        "        return unrenumber(df, col, renum)\n",
        "    except Exception:\n",
        "        return df  # fallback\n",
        "\n",
        "def get_edgelist_df(G_cu):\n",
        "    \"\"\"\n",
        "    Fetch a cuDF edgelist from a cuGraph Graph across RAPIDS versions and\n",
        "    normalize column names to: 'src', 'dst', 'weight'.\n",
        "\n",
        "    Robust to odd orders/names like ['weight','ORIGIN','DEST'] or\n",
        "    absence of explicit 'weight' (creates unit weights).\n",
        "    \"\"\"\n",
        "    import cudf\n",
        "\n",
        "    # 1) Try current accessor\n",
        "    df = None\n",
        "    try:\n",
        "        df = G_cu.view_edge_list()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Try legacy accessor\n",
        "    if df is None:\n",
        "        try:\n",
        "            df = G_cu.edgelist.edgelist_df\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Cannot retrieve cuGraph edgelist\") from e\n",
        "\n",
        "    cols = list(df.columns)\n",
        "\n",
        "    # Candidates to recognize by name\n",
        "    src_candidates = {\n",
        "        \"src\",\"source\",\"sources\",\"from\",\"source_vertex\",\"source_id\",\n",
        "        \"src_0\",\"ORIGIN\",\"Origin\",\"origin\"\n",
        "    }\n",
        "    dst_candidates = {\n",
        "        \"dst\",\"destination\",\"destinations\",\"to\",\"dest_vertex\",\"dest_id\",\n",
        "        \"dst_0\",\"DEST\",\"Dest\",\"dest\"\n",
        "    }\n",
        "    w_candidates = {\"weight\",\"weights\",\"edge_weight\",\"w\",\"W\",\"value\",\"WEIGHT\"}\n",
        "\n",
        "    # Identify weight by name first\n",
        "    w_col = next((c for c in cols if c in w_candidates), None)\n",
        "\n",
        "    # If no explicit weight, try to infer:\n",
        "    if w_col is None:\n",
        "        # Heuristic: if exactly 3 cols and one looks numeric, pick the numeric one.\n",
        "        if len(cols) == 3:\n",
        "            numeric_like = []\n",
        "            for c in cols:\n",
        "                try:\n",
        "                    # cuDF dtype kind 'f'/'i'/'u' are numeric; fall back to try astype\n",
        "                    if str(df[c].dtype).startswith((\"float\",\"int\",\"uint\")):\n",
        "                        numeric_like.append(c)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if len(numeric_like) == 1:\n",
        "                w_col = numeric_like[0]\n",
        "\n",
        "    # If still none, create a unit weight\n",
        "    if w_col is None:\n",
        "        df = df.assign(weight=1.0)\n",
        "        w_col = \"weight\"\n",
        "\n",
        "    # Now find src/dst by name\n",
        "    src_col = next((c for c in cols if c in src_candidates), None)\n",
        "    dst_col = next((c for c in cols if c in dst_candidates), None)\n",
        "\n",
        "    # If either is missing, infer by \"the two non-weight columns\"\n",
        "    if src_col is None or dst_col is None:\n",
        "        non_weight = [c for c in cols if c != w_col]\n",
        "        if len(non_weight) < 2:\n",
        "            raise RuntimeError(f\"Cannot infer src/dst; edgelist columns={cols}\")\n",
        "        # Keep original order among non-weight columns; treat first as src, second as dst\n",
        "        src_col, dst_col = non_weight[0], non_weight[1]\n",
        "\n",
        "    # Normalize names\n",
        "    df_norm = df.rename(columns={src_col: \"src\", dst_col: \"dst\", w_col: \"weight\"})\n",
        "    # Keep only needed cols in canonical order\n",
        "    return df_norm[[\"src\", \"dst\", \"weight\"]]\n",
        "\n",
        "\n",
        "\n",
        "def compute_centralities_gpu(G_cu,\n",
        "                             renum_map=None,\n",
        "                             pagerank_alpha: float = 0.85,\n",
        "                             k_sources: int = 128) -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    GPU centralities using cuGraph + cuDF with version-robust edgelist handling.\n",
        "    - Weighted degree/in/out via cuDF groupbys on normalized edgelist (src,dst,weight)\n",
        "    - PageRank / Betweenness / Eigenvector via cuGraph\n",
        "    - Unrenumber outputs back to original IDs when renum_map is provided\n",
        "    \"\"\"\n",
        "    import cudf, cugraph\n",
        "\n",
        "    out = {}\n",
        "\n",
        "    # --- Normalize the edgelist view (handles all RAPIDS variants) ---\n",
        "    view = get_edgelist_df(G_cu)  # columns: src, dst, weight\n",
        "    print(f\"GPU centralities: using edgelist columns {list(view.columns)}\")\n",
        "\n",
        "    # --- Weighted degrees via cuDF groupby ---\n",
        "    out_deg = view.groupby(\"src\")[\"weight\"].sum().reset_index().rename(columns={\"weight\": \"out_degree\", \"src\": \"vertex\"})\n",
        "    in_deg  = view.groupby(\"dst\")[\"weight\"].sum().reset_index().rename(columns={\"weight\": \"in_degree\",  \"dst\": \"vertex\"})\n",
        "    deg_df  = out_deg.merge(in_deg, on=\"vertex\", how=\"outer\").fillna(0.0)\n",
        "    deg_df[\"degree\"] = deg_df[\"in_degree\"] + deg_df[\"out_degree\"]\n",
        "\n",
        "    # Unrenumber util (safe no-op if not available)\n",
        "    def _unrenumber_df(df, col, renum):\n",
        "        try:\n",
        "            from cugraph.utilities import unrenumber\n",
        "            return unrenumber(df, col, renum)\n",
        "        except Exception:\n",
        "            return df\n",
        "\n",
        "    deg_df  = _unrenumber_df(deg_df,  \"vertex\", renum_map)\n",
        "    out_deg = _unrenumber_df(out_deg, \"vertex\", renum_map)\n",
        "    in_deg  = _unrenumber_df(in_deg,  \"vertex\", renum_map)\n",
        "\n",
        "    out[\"degree\"]     = dict(zip(deg_df[\"vertex\"].to_pandas(),  deg_df[\"degree\"].to_pandas()))\n",
        "    out[\"in_degree\"]  = dict(zip(in_deg[\"vertex\"].to_pandas(),  in_deg[\"in_degree\"].to_pandas()))\n",
        "    out[\"out_degree\"] = dict(zip(out_deg[\"vertex\"].to_pandas(), out_deg[\"out_degree\"].to_pandas()))\n",
        "\n",
        "    # --- PageRank ---\n",
        "    print(\"GPU centralities: pagerank ...\")\n",
        "    pr_df = cugraph.pagerank(G_cu, alpha=pagerank_alpha)\n",
        "    pr_df = _unrenumber_df(pr_df, \"vertex\", renum_map)\n",
        "    out[\"pagerank\"] = dict(zip(pr_df[\"vertex\"].to_pandas(), pr_df[\"pagerank\"].to_pandas()))\n",
        "\n",
        "    # --- Betweenness (approx with k sources) ---\n",
        "    print(f\"GPU centralities: betweenness approx (k={k_sources}) ...\")\n",
        "    try:\n",
        "        k = min(k_sources, int(G_cu.number_of_vertices()))\n",
        "        bc_df = cugraph.betweenness_centrality(G_cu, k=k, normalized=True)\n",
        "        bc_df = _unrenumber_df(bc_df, \"vertex\", renum_map)\n",
        "        bc_col = \"betweenness_centrality\" if \"betweenness_centrality\" in bc_df.columns else (\n",
        "                 \"betweenness\" if \"betweenness\" in bc_df.columns else bc_df.columns[-1])\n",
        "        out[\"betweenness\"] = dict(zip(bc_df[\"vertex\"].to_pandas(), bc_df[bc_col].to_pandas()))\n",
        "    except Exception:\n",
        "        out[\"betweenness\"] = {}\n",
        "\n",
        "    # --- Eigenvector (needs undirected proxy) ---\n",
        "    print(\"GPU centralities: eigenvector (undirected proxy) ...\")\n",
        "    try:\n",
        "        Gu = cugraph.Graph(directed=False)\n",
        "        Gu.from_cudf_edgelist(view, source=\"src\", destination=\"dst\", edge_attr=\"weight\", renumber=False)\n",
        "        ev_df = cugraph.eigenvector_centrality(Gu)\n",
        "        ev_df = _unrenumber_df(ev_df, \"vertex\", renum_map)\n",
        "        ev_col = \"eigenvector_centrality\" if \"eigenvector_centrality\" in ev_df.columns else ev_df.columns[-1]\n",
        "        out[\"eigenvector\"] = dict(zip(ev_df[\"vertex\"].to_pandas(), ev_df[ev_col].to_pandas()))\n",
        "    except Exception:\n",
        "        out[\"eigenvector\"] = {}\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "# --------------- CENTRALITIES (CPU) ---------------\n",
        "def compute_centralities_cpu(G: nx.DiGraph,\n",
        "                             use_approx_betw=True,\n",
        "                             k_sources=K_BETWEENNESS,\n",
        "                             pagerank_alpha=0.85) -> Dict[str, Dict]:\n",
        "    w = \"weight\"\n",
        "    print(\"CPU centralities: degree/in/out ...\")\n",
        "    deg   = dict(G.degree(weight=w))\n",
        "    indeg = dict(G.in_degree(weight=w))\n",
        "    outd  = dict(G.out_degree(weight=w))\n",
        "    if use_approx_betw:\n",
        "        print(f\"CPU centralities: betweenness approx (k={k_sources}) ...\")\n",
        "        nodes = list(G.nodes)\n",
        "        if len(nodes) == 0:\n",
        "            src = []\n",
        "        else:\n",
        "            rng = np.random.default_rng(123)\n",
        "            src = list(rng.choice(nodes, size=min(k_sources, len(nodes)), replace=False))\n",
        "        try:\n",
        "            btw = nx.betweenness_centrality_subset(G, sources=src, targets=list(G.nodes), normalized=True, weight=w)\n",
        "        except Exception:\n",
        "            btw = {n: np.nan for n in G.nodes}\n",
        "    else:\n",
        "        print(\"CPU centralities: betweenness exact (slow) ...\")\n",
        "        btw = nx.betweenness_centrality(G, weight=w, normalized=True)\n",
        "    print(\"CPU centralities: eigenvector (undirected proxy) ...\")\n",
        "    try:\n",
        "        ev = nx.eigenvector_centrality_numpy(G.to_undirected(), weight=w)\n",
        "    except Exception:\n",
        "        ev = {n: np.nan for n in G.nodes}\n",
        "    print(\"CPU centralities: pagerank ...\")\n",
        "    try:\n",
        "        pr = nx.pagerank(G, alpha=pagerank_alpha, weight=w)\n",
        "    except Exception:\n",
        "        pr = {n: np.nan for n in G.nodes}\n",
        "    return {\"degree\":deg, \"in_degree\":indeg, \"out_degree\":outd,\n",
        "            \"betweenness\":btw, \"eigenvector\":ev, \"pagerank\":pr}\n",
        "\n",
        "def ensure_common_nodes(G_pre: nx.DiGraph, G_post: nx.DiGraph) -> List:\n",
        "    nodes = sorted(set(G_pre.nodes()) | set(G_post.nodes()))\n",
        "    for n in nodes:\n",
        "        if n not in G_pre:  G_pre.add_node(n)\n",
        "        if n not in G_post: G_post.add_node(n)\n",
        "    return nodes\n",
        "\n",
        "def paired_centrality_tests(G_pre, G_post, use_gpu=False, metrics=None) -> pd.DataFrame:\n",
        "    print(\"[1/4] Paired centrality tests: start\")\n",
        "    t0 = time.time()\n",
        "    if metrics is None:\n",
        "        metrics = [\"degree\",\"in_degree\",\"out_degree\",\"betweenness\",\"eigenvector\",\"pagerank\"]\n",
        "\n",
        "    if use_gpu and HAS_RAPIDS:\n",
        "        import cudf\n",
        "        print(\"  - building cuGraph views (with renumber=True) ...\")\n",
        "        # Convert NetworkX graphs to cuDF\n",
        "        def nx_to_cudf(Gnx):\n",
        "            src, dst, w = [], [], []\n",
        "            for u, v, d in Gnx.edges(data=True):\n",
        "                src.append(u); dst.append(v); w.append(d.get(\"weight\", 1.0))\n",
        "            return cudf.DataFrame({\"ORIGIN\":src, \"DEST\":dst, \"W\":w})\n",
        "        df_pre  = nx_to_cudf(G_pre)\n",
        "        df_post = nx_to_cudf(G_post)\n",
        "        G0, ren0 = build_cugraph_digraph(df_pre.rename(columns={\"W\":\"W_PRE\"}),  \"W_PRE\")\n",
        "        G1, ren1 = build_cugraph_digraph(df_post.rename(columns={\"W\":\"W_POST\"}), \"W_POST\")\n",
        "        Cpre  = compute_centralities_gpu(G0, renum_map=ren0, k_sources=K_BETWEENNESS)\n",
        "        Cpost = compute_centralities_gpu(G1, renum_map=ren1, k_sources=K_BETWEENNESS)\n",
        "        nodes = set()\n",
        "        for key in (\"degree\",\"pagerank\",\"in_degree\",\"out_degree\",\"betweenness\",\"eigenvector\"):\n",
        "            nodes |= set(Cpre.get(key,{}).keys()) | set(Cpost.get(key,{}).keys())\n",
        "        nodes = sorted(nodes)\n",
        "    else:\n",
        "        nodes = ensure_common_nodes(G_pre, G_post)\n",
        "        print(f\"  - nodes in union: {len(nodes)}\")\n",
        "        print(\"  - computing centralities (Pre)\")\n",
        "        Cpre  = compute_centralities_cpu(G_pre, use_approx_betw=True, k_sources=K_BETWEENNESS)\n",
        "        print(\"  - computing centralities (Post)\")\n",
        "        Cpost = compute_centralities_cpu(G_post, use_approx_betw=True, k_sources=K_BETWEENNESS)\n",
        "\n",
        "    rows=[]\n",
        "    for m in metrics:\n",
        "        print(f\"  - testing metric: {m}\")\n",
        "        x = np.array([Cpre.get(m,{}).get(n, np.nan)  for n in nodes], dtype=float)\n",
        "        y = np.array([Cpost.get(m,{}).get(n, np.nan) for n in nodes], dtype=float)\n",
        "        mask = np.isfinite(x) & np.isfinite(y)\n",
        "        x, y = x[mask], y[mask]\n",
        "        if len(x) < 3:\n",
        "            rows.append({\"metric\":m,\"n\":len(x),\"t\":np.nan,\"p_t\":np.nan,\"dz\":np.nan,\n",
        "                         \"wilcoxon_stat\":np.nan,\"p_w\":np.nan})\n",
        "            continue\n",
        "        t_stat, p_t = stats.ttest_rel(x, y, nan_policy=\"omit\")\n",
        "        d = y - x\n",
        "        dz = np.mean(d) / (np.std(d, ddof=1) + 1e-12)  # Cohen's dz\n",
        "        try:\n",
        "            w_stat, p_w = stats.wilcoxon(x, y, zero_method=\"wilcox\", correction=False)\n",
        "        except Exception:\n",
        "            w_stat, p_w = np.nan, np.nan\n",
        "        rows.append({\"metric\":m,\"n\":len(x),\"t\":t_stat,\"p_t\":p_t,\"dz\":dz,\n",
        "                     \"wilcoxon_stat\":w_stat,\"p_w\":p_w})\n",
        "    print(f\"[1/4] Paired centrality tests: done in {time.time()-t0:.1f}s\")\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# --------------- NETWORK METRICS ---------------\n",
        "def network_level_metrics(G: nx.DiGraph) -> Dict[str, float]:\n",
        "    res={}\n",
        "    res[\"n_nodes\"] = G.number_of_nodes()\n",
        "    res[\"n_edges\"] = G.number_of_edges()\n",
        "    res[\"density\"] = nx.density(G)\n",
        "    if G.number_of_nodes()==0:\n",
        "        return res\n",
        "    comp_nodes = max(nx.weakly_connected_components(G), key=len) if G.is_directed() else max(nx.connected_components(G), key=len)\n",
        "    H = G.subgraph(comp_nodes).copy()\n",
        "    try:\n",
        "        res[\"transitivity\"]   = nx.transitivity(H.to_undirected())\n",
        "        res[\"avg_clustering\"] = nx.average_clustering(H.to_undirected(), weight=\"weight\")\n",
        "    except Exception:\n",
        "        res[\"transitivity\"] = np.nan; res[\"avg_clustering\"] = np.nan\n",
        "    try:\n",
        "        W = H.copy()\n",
        "        for u,v,data in W.edges(data=True):\n",
        "            w = data.get(\"weight\",1.0)\n",
        "            data[\"cost\"] = 1.0/w if w>0 else 1e9\n",
        "        res[\"aspl_cost\"] = nx.average_shortest_path_length(W, weight=\"cost\") if nx.is_weakly_connected(W) else np.nan\n",
        "    except Exception:\n",
        "        res[\"aspl_cost\"] = np.nan\n",
        "    try:\n",
        "        res[\"assortativity_deg\"] = nx.degree_assortativity_coefficient(H.to_undirected())\n",
        "    except Exception:\n",
        "        res[\"assortativity_deg\"] = np.nan\n",
        "    if _HAS_COMMUNITY:\n",
        "        try:\n",
        "            part = community.best_partition(H.to_undirected(), weight=\"weight\")\n",
        "            res[\"modularity\"] = community.modularity(part, H.to_undirected(), weight=\"weight\")\n",
        "        except Exception:\n",
        "            res[\"modularity\"] = np.nan\n",
        "    else:\n",
        "        res[\"modularity\"] = np.nan\n",
        "    return res\n",
        "\n",
        "def bootstrap_metric_diff(edges: pd.DataFrame,\n",
        "                          metric_fn: Callable[[nx.DiGraph], float],\n",
        "                          n_boot: int = N_BOOT,\n",
        "                          s_col: str = \"ORIGIN\",\n",
        "                          t_col: str = \"DEST\",\n",
        "                          seed: int = 42) -> Dict[str, float]:\n",
        "    print(f\"  - bootstrap CI ({metric_fn.__name__}): n_boot={n_boot}\")\n",
        "    rng = np.random.default_rng(seed)\n",
        "    G0 = build_weighted_digraph_pandas(edges, \"W_PRE\",  s_col, t_col)\n",
        "    G1 = build_weighted_digraph_pandas(edges, \"W_POST\", s_col, t_col)\n",
        "    diff_obs = metric_fn(G1) - metric_fn(G0)\n",
        "    diffs=[]; idx=np.arange(len(edges))\n",
        "    t0 = time.time()\n",
        "    for b in range(n_boot):\n",
        "        if (b+1) % PRINT_EVERY == 0:\n",
        "            print(f\"    * bootstrap {b+1}/{n_boot} (elapsed {time.time()-t0:.1f}s)\")\n",
        "        samp = edges.iloc[rng.choice(idx, size=len(idx), replace=True)]\n",
        "        Gp = build_weighted_digraph_pandas(samp, \"W_PRE\",  s_col, t_col)\n",
        "        Gq = build_weighted_digraph_pandas(samp, \"W_POST\", s_col, t_col)\n",
        "        diffs.append(metric_fn(Gq) - metric_fn(Gp))\n",
        "    diffs = np.array(diffs, dtype=float)\n",
        "    lo,hi = np.quantile(diffs,[0.025,0.975])\n",
        "    return {\"diff_obs\": float(diff_obs), \"ci95_lo\": float(lo), \"ci95_hi\": float(hi)}\n",
        "\n",
        "def perm_test_network_metric(edges: pd.DataFrame,\n",
        "                             metric_fn: Callable[[nx.DiGraph], float],\n",
        "                             n_perm: int = N_PERM,\n",
        "                             s_col: str = \"ORIGIN\",\n",
        "                             t_col: str = \"DEST\",\n",
        "                             seed: int = 123) -> Dict[str, float]:\n",
        "    print(f\"  - permutation test ({metric_fn.__name__}): n_perm={n_perm}\")\n",
        "    rng = np.random.default_rng(seed)\n",
        "    G0 = build_weighted_digraph_pandas(edges, \"W_PRE\",  s_col, t_col)\n",
        "    G1 = build_weighted_digraph_pandas(edges, \"W_POST\", s_col, t_col)\n",
        "    diff_obs = metric_fn(G1) - metric_fn(G0)\n",
        "    diffs = np.empty(n_perm, dtype=float)\n",
        "    base = edges[[\"ORIGIN\",\"DEST\",\"W_PRE\",\"W_POST\"]].to_numpy(copy=True)\n",
        "    t0 = time.time()\n",
        "    for i in range(n_perm):\n",
        "        if (i+1) % PRINT_EVERY == 0:\n",
        "            print(f\"    * perm {i+1}/{n_perm} (elapsed {time.time()-t0:.1f}s)\")\n",
        "        arr = base.copy()\n",
        "        swaps = rng.random(len(arr)) < 0.5\n",
        "        arr[swaps, [2,3]] = arr[swaps, [3,2]]  # swap per edge\n",
        "        eperm = pd.DataFrame(arr, columns=[\"ORIGIN\",\"DEST\",\"W_PRE\",\"W_POST\"])\n",
        "        Gp = build_weighted_digraph_pandas(eperm, \"W_PRE\",  s_col, t_col)\n",
        "        Gq = build_weighted_digraph_pandas(eperm, \"W_POST\", s_col, t_col)\n",
        "        diffs[i] = metric_fn(Gq) - metric_fn(Gp)\n",
        "    p_two = (1.0 + np.sum(np.abs(diffs) >= np.abs(diff_obs))) / (n_perm + 1.0)\n",
        "    return {\"diff_obs\": float(diff_obs), \"p_two_sided\": float(p_two)}\n",
        "\n",
        "# Metric wrappers\n",
        "def metric_avg_clustering(G): return network_level_metrics(G).get(\"avg_clustering\", np.nan)\n",
        "def metric_aspl_cost(G):     return network_level_metrics(G).get(\"aspl_cost\", np.nan)\n",
        "def metric_modularity(G):    return network_level_metrics(G).get(\"modularity\", np.nan)\n",
        "\n",
        "# --------------- SEM ---------------\n",
        "def run_sem_node_level(G_pre: nx.DiGraph, G_post: nx.DiGraph) -> Dict:\n",
        "    print(\"[4/4] SEM: start\")\n",
        "    t0 = time.time()\n",
        "    try:\n",
        "        import semopy\n",
        "        print(\"  - semopy found\")\n",
        "    except Exception:\n",
        "        print(\"  - installing semopy ...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"semopy\"], check=False)\n",
        "        import semopy\n",
        "        print(\"  - semopy installed\")\n",
        "\n",
        "    nodes = ensure_common_nodes(G_pre, G_post)\n",
        "    print(f\"  - SEM nodes: {len(nodes)} (features)\")\n",
        "\n",
        "    def node_strength(G):\n",
        "        s_in  = dict(G.in_degree(weight=\"weight\"))\n",
        "        s_out = dict(G.out_degree(weight=\"weight\"))\n",
        "        return {n: s_in.get(n,0.0) + s_out.get(n,0.0) for n in nodes}\n",
        "\n",
        "    try:\n",
        "        ev_pre  = nx.eigenvector_centrality_numpy(G_pre.to_undirected(),  weight=\"weight\")\n",
        "        ev_post = nx.eigenvector_centrality_numpy(G_post.to_undirected(), weight=\"weight\")\n",
        "    except Exception:\n",
        "        ev_pre  = {n: np.nan for n in nodes}\n",
        "        ev_post = {n: np.nan for n in nodes}\n",
        "\n",
        "    s_pre  = node_strength(G_pre)\n",
        "    s_post = node_strength(G_post)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"node\": nodes,\n",
        "        \"centrality_pre\":  [ev_pre.get(n, np.nan)  for n in nodes],\n",
        "        \"centrality_post\": [ev_post.get(n, np.nan) for n in nodes],\n",
        "        \"strength_pre\":    [s_pre.get(n, 0.0)      for n in nodes],\n",
        "        \"strength_post\":   [s_post.get(n, 0.0)     for n in nodes],\n",
        "    })\n",
        "    df[\"centrality_change\"] = df[\"centrality_post\"] - df[\"centrality_pre\"]\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        df[\"growth\"] = 100.0 * (df[\"strength_post\"] - df[\"strength_pre\"]) / np.where(df[\"strength_pre\"]>0, df[\"strength_pre\"], np.nan)\n",
        "\n",
        "    X = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"centrality_pre\",\"centrality_post\",\"centrality_change\",\"strength_pre\",\"growth\"]).copy()\n",
        "    print(f\"  - SEM rows used: {len(X)}\")\n",
        "\n",
        "    # Standardize numeric cols\n",
        "    for c in X.select_dtypes(include=[np.number]).columns:\n",
        "        s = X[c].std(ddof=0)\n",
        "        if s>0: X[c] = (X[c] - X[c].mean())/s\n",
        "\n",
        "    model_desc = \"\"\"\n",
        "    centrality_post ~ centrality_pre + strength_pre\n",
        "    growth          ~ centrality_change + strength_pre\n",
        "    \"\"\"\n",
        "    model = semopy.Model(model_desc)\n",
        "    opt   = semopy.Optimizer(model)\n",
        "    opt.optimize(X)\n",
        "    est = model.inspect(std_est=True)\n",
        "    try:\n",
        "        fit = semopy.calc_stats(model, X)\n",
        "        fit_keep = {k: fit.get(k) for k in (\"AIC\",\"BIC\",\"CFI\",\"TLI\",\"RMSEA\",\"SRMR\",\"GFI\",\"AGFI\")}\n",
        "    except Exception:\n",
        "        fit_keep = {}\n",
        "    print(f\"[4/4] SEM: done in {time.time()-t0:.1f}s\")\n",
        "    return {\"data_used_n\": int(len(X)), \"estimates\": est.to_dict(orient=\"records\"), \"fit\": fit_keep}\n",
        "\n",
        "# --------------- REPORT UTILS ---------------\n",
        "def fmt(x, nd=3):\n",
        "    if x is None or (isinstance(x,float) and (np.isnan(x) or not np.isfinite(x))): return \"NA\"\n",
        "    return f\"{x:.{nd}f}\"\n",
        "\n",
        "def fmt_p(p):\n",
        "    if p is None or (isinstance(p,float) and (np.isnan(p) or not np.isfinite(p))): return \"p=NA\"\n",
        "    if p < 1e-4: return \"p<1e-4\"\n",
        "    return f\"p={p:.4f}\"\n",
        "\n",
        "def section(title): return f\"\\n## {title}\\n\"\n",
        "\n",
        "# ---------------------- MAIN ----------------------\n",
        "def main():\n",
        "    ensure_dir(OUTPUT_DIR)\n",
        "    assert os.path.exists(INPUT_EDGES), f\"Edges file not found: {INPUT_EDGES}\"\n",
        "    print(f\"Reading edges: {INPUT_EDGES}\")\n",
        "    edges = pd.read_csv(INPUT_EDGES)\n",
        "    print(f\"  - rows: {len(edges)} | columns: {list(edges.columns)}\")\n",
        "\n",
        "    # Build CPU graphs\n",
        "    print(\"Building CPU graphs ...\")\n",
        "    t0 = time.time()\n",
        "    G_pre  = build_weighted_digraph_pandas(edges, \"W_PRE\")\n",
        "    G_post = build_weighted_digraph_pandas(edges, \"W_POST\")\n",
        "    print(f\"  - G_pre: nodes={G_pre.number_of_nodes()}, edges={G_pre.number_of_edges()}\")\n",
        "    print(f\"  - G_post: nodes={G_post.number_of_nodes()}, edges={G_post.number_of_edges()}\")\n",
        "    print(f\"Graphs built in {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # 1) Paired centrality tests (GPU if RAPIDS available, using renumber/unrenumber)\n",
        "    ct_df = paired_centrality_tests(G_pre, G_post, use_gpu=HAS_RAPIDS)\n",
        "\n",
        "    # 2) Network-level metrics + bootstrap CIs\n",
        "    print(\"[2/4] Network-level metrics (baseline) ...\")\n",
        "    t2 = time.time()\n",
        "    metrics_then = network_level_metrics(G_pre)\n",
        "    metrics_now  = network_level_metrics(G_post)\n",
        "    print(f\"  - baseline computed in {time.time()-t2:.1f}s\")\n",
        "\n",
        "    print(\"[2/4] Bootstrap CIs ...\")\n",
        "    t2 = time.time()\n",
        "    boots = {\n",
        "        \"avg_clustering\": bootstrap_metric_diff(edges, metric_avg_clustering, n_boot=N_BOOT),\n",
        "        \"aspl_cost\":      bootstrap_metric_diff(edges, metric_aspl_cost,      n_boot=N_BOOT),\n",
        "        \"modularity\":     bootstrap_metric_diff(edges, metric_modularity,     n_boot=N_BOOT) if _HAS_COMMUNITY else {\"diff_obs\": np.nan, \"ci95_lo\": np.nan, \"ci95_hi\": np.nan}\n",
        "    }\n",
        "    print(f\"[2/4] Bootstrap done in {time.time()-t2:.1f}s\")\n",
        "\n",
        "    # 3) Permutation tests\n",
        "    print(\"[3/4] Permutation tests ...\")\n",
        "    t3 = time.time()\n",
        "    perms = {\n",
        "        \"avg_clustering\": perm_test_network_metric(edges, metric_avg_clustering, n_perm=N_PERM),\n",
        "        \"aspl_cost\":      perm_test_network_metric(edges, metric_aspl_cost,      n_perm=N_PERM),\n",
        "        \"modularity\":     perm_test_network_metric(edges, metric_modularity,     n_perm=N_PERM) if _HAS_COMMUNITY else {\"diff_obs\": np.nan, \"p_two_sided\": np.nan}\n",
        "    }\n",
        "    print(f\"[3/4] Permutation done in {time.time()-t3:.1f}s\")\n",
        "\n",
        "    # Plain-English summaries\n",
        "    try:\n",
        "        top_change = ct_df.sort_values(\"dz\", key=lambda s: s.abs(), ascending=False).iloc[0]\n",
        "        plain_ct = f\"The biggest shift was in **{top_change['metric']}** (effect size dzâ‰ˆ{fmt(top_change['dz'])}).\"\n",
        "    except Exception:\n",
        "        plain_ct = \"Centrality changes were modest overall.\"\n",
        "\n",
        "    plain_boot = []\n",
        "    for name, res in boots.items():\n",
        "        if all(np.isfinite([res.get(\"diff_obs\",np.nan), res.get(\"ci95_lo\",np.nan), res.get(\"ci95_hi\",np.nan)])):\n",
        "            crosses = res[\"ci95_lo\"] <= 0.0 <= res[\"ci95_hi\"]\n",
        "            plain_boot.append(f\"- **{name}** changed by {fmt(res['diff_obs'])} (95% CI [{fmt(res['ci95_lo'])}, {fmt(res['ci95_hi'])}]) â€” {'uncertain (CI crosses 0)' if crosses else 'likely real (CI excludes 0)'}\")\n",
        "        else:\n",
        "            plain_boot.append(f\"- **{name}**: CI not available.\")\n",
        "    plain_boot = \"\\n\".join(plain_boot)\n",
        "\n",
        "    plain_perm = []\n",
        "    for name, res in perms.items():\n",
        "        ptxt = fmt_p(res.get(\"p_two_sided\", np.nan))\n",
        "        plain_perm.append(f\"- **{name}**: observed change={fmt(res.get('diff_obs',np.nan))}, {ptxt} â†’ {'unlikely by chance' if (isinstance(res.get('p_two_sided',np.nan),float) and res['p_two_sided']<ALPHA) else 'could be chance variation'}\")\n",
        "    plain_perm = \"\\n\".join(plain_perm)\n",
        "\n",
        "    # 4) SEM\n",
        "    sem_out = run_sem_node_level(G_pre, G_post)\n",
        "    sem_plain = f\"SEM used {sem_out['data_used_n']} nodes. Paths estimate how prior connectivity relates to later connectivity and growth; significant positive coefficients indicate airports with higher starting centrality (or strength) tended to remain central or grow more.\"\n",
        "\n",
        "    # Save JSON summary\n",
        "    summary = {\n",
        "        \"inputs\": {\"edges_path\": INPUT_EDGES},\n",
        "        \"centrality_tests\": ct_df.to_dict(orient=\"records\"),\n",
        "        \"network_metrics_pre\": metrics_then,\n",
        "        \"network_metrics_post\": metrics_now,\n",
        "        \"bootstrap_diffs\": boots,\n",
        "        \"perm_tests\": perms,\n",
        "        \"sem\": sem_out,\n",
        "        \"notes\": \"GPU centralities use renumber=True and unrenumbered back to original node ids.\"\n",
        "    }\n",
        "    out_json = os.path.join(OUTPUT_DIR, \"hyp3_summary.json\")\n",
        "    with open(out_json, \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"Wrote JSON: {out_json}\")\n",
        "\n",
        "    # Build Markdown report\n",
        "    md = []\n",
        "    md.append(\"# Hypothesis 3 â€” Network Inversion Analyses\\n\")\n",
        "    md.append(f\"- **Edges source:** `{INPUT_EDGES}`\")\n",
        "    md.append(f\"- **GPU:** {'ON (RAPIDS)' if HAS_RAPIDS else ('ON (no RAPIDS)' if HAS_GPU else 'OFF')}  |  **RAM:** {('%.1f GB' % ram_gb) if ram_gb else 'NA'}\\n\")\n",
        "\n",
        "    md.append(section(\"Paired t-tests on Node Centralities (Pre vs Post)\"))\n",
        "    md.append(pd.DataFrame(summary[\"centrality_tests\"]).to_markdown(index=False))\n",
        "    md.append(\"**Technical interpretation:** For each centrality metric, paired t-tests (and Wilcoxon) test whether nodes changed from Pre to Post; Cohenâ€™s dz summarizes effect size.\\n\")\n",
        "    md.append(f\"**In plain English:** {plain_ct}\\n\")\n",
        "\n",
        "    md.append(section(\"Network-Level Metrics (Pre vs Post Baseline)\"))\n",
        "    pre_tbl  = pd.DataFrame(summary[\"network_metrics_pre\"],  index=[\"Pre\"]).T.rename(columns={\"Pre\":\"value\"})\n",
        "    post_tbl = pd.DataFrame(summary[\"network_metrics_post\"], index=[\"Post\"]).T.rename(columns={\"Post\":\"value\"})\n",
        "    md.append(\"**Pre (baseline):**\");  md.append(pre_tbl.to_markdown())\n",
        "    md.append(\"**Post:**\");            md.append(post_tbl.to_markdown())\n",
        "    md.append(\"**Technical interpretation:** Density, clustering, path length, assortativity, and modularity describe global structure.\\n\")\n",
        "    md.append(\"**In plain English:** Compare Pre vs Post to see if the network became more connected (higher clustering/density) or more streamlined (lower path cost).\\n\")\n",
        "\n",
        "    md.append(section(\"Bootstrap CIs for Metric Differences (Post âˆ’ Pre)\"))\n",
        "    md.append(pd.DataFrame(summary[\"bootstrap_diffs\"]).T.to_markdown())\n",
        "    md.append(\"**Technical interpretation:** Nonparametric bootstrap resamples OD pairs; CI excluding 0 implies a likely real change.\\n\")\n",
        "    md.append(f\"**In plain English:**\\n{plain_boot}\\n\")\n",
        "\n",
        "    md.append(section(\"Permutation Tests (Paired Label-Swap)\"))\n",
        "    md.append(pd.DataFrame(summary[\"perm_tests\"]).T.to_markdown())\n",
        "    md.append(\"**Technical interpretation:** Swapping (W_PRE,W_POST) per OD forms a null distribution; two-sided p-value tests if the observed difference is unusual.\\n\")\n",
        "    md.append(f\"**In plain English:**\\n{plain_perm}\\n\")\n",
        "\n",
        "    md.append(section(\"Structural Equation Modeling (SEM) â€” Node-Level\"))\n",
        "    est_df = pd.DataFrame(sem_out[\"estimates\"])\n",
        "    if not est_df.empty:\n",
        "        md.append(\"**Parameter estimates (standardized):**\")\n",
        "        md.append(est_df.to_markdown(index=False))\n",
        "    else:\n",
        "        md.append(\"_No SEM estimates available (insufficient data after cleaning)._\")\n",
        "    fit_df = pd.DataFrame([sem_out.get(\"fit\", {})]).T\n",
        "    if not fit_df.empty:\n",
        "        md.append(\"**Fit indices:**\"); md.append(fit_df.to_markdown(header=False))\n",
        "    md.append(\"**Technical interpretation:** Paths from centrality_pre/strength_pre to centrality_post test persistence; paths to growth test whether centrality changes relate to traffic growth.\\n\")\n",
        "    md.append(f\"**In plain English:** {sem_plain}\\n\")\n",
        "\n",
        "    out_md = os.path.join(OUTPUT_DIR, \"hyp3_stats_report.md\")\n",
        "    with open(out_md, \"w\") as f:\n",
        "        f.write(\"\\n\".join(md))\n",
        "    print(f\"Wrote MD: {out_md}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    t_all = time.time()\n",
        "    main()\n",
        "    print(f\"ALL DONE in {time.time()-t_all:.1f}s\")\n"
      ],
      "metadata": {
        "id": "7bEGQ6zwmS8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9bd3e9-158f-4dc7-9188-af030702f2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Environment ===\n",
            "RAM: 56.9 GB\n",
            "GPU detected:\n",
            " Tue Sep  2 00:03:13 2025       \n",
            "Auto-tuned for GPU/high-RAM: N_BOOT=1000, N_PERM=2000, K_BETWEENNESS=128\n",
            "RAPIDS (cuDF/cuGraph) already available.\n",
            "Reading edges: /content/drive/MyDrive/airline_data_analysis_v2/hyp3_outputs/h3_edges_pre_post.csv\n",
            "  - rows: 40010 | columns: ['ORIGIN', 'DEST', 'W_PRE', 'W_POST', 'GROWTH_ABS', 'GROWTH_PCT', 'ORIGIN_HUB_TYPE', 'DEST_HUB_TYPE']\n",
            "Building CPU graphs ...\n",
            "  - G_pre: nodes=1632, edges=39957\n",
            "  - G_post: nodes=1632, edges=39957\n",
            "Graphs built in 0.4s\n",
            "[1/4] Paired centrality tests: start\n",
            "  - building cuGraph views (with renumber=True) ...\n",
            "GPU centralities: using edgelist columns ['src', 'dst', 'weight']\n",
            "GPU centralities: pagerank ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cugraph/link_analysis/pagerank.py:232: UserWarning: Pagerank expects the 'store_transposed' flag to be set to 'True' for optimal performance during the graph creation\n",
            "  warnings.warn(warning_msg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU centralities: betweenness approx (k=128) ...\n",
            "GPU centralities: eigenvector (undirected proxy) ...\n",
            "GPU centralities: using edgelist columns ['src', 'dst', 'weight']\n",
            "GPU centralities: pagerank ...\n",
            "GPU centralities: betweenness approx (k=128) ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/cugraph/link_analysis/pagerank.py:232: UserWarning: Pagerank expects the 'store_transposed' flag to be set to 'True' for optimal performance during the graph creation\n",
            "  warnings.warn(warning_msg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU centralities: eigenvector (undirected proxy) ...\n",
            "  - testing metric: degree\n",
            "  - testing metric: in_degree\n",
            "  - testing metric: out_degree\n",
            "  - testing metric: betweenness\n",
            "  - testing metric: eigenvector\n",
            "  - testing metric: pagerank\n",
            "[1/4] Paired centrality tests: done in 1.2s\n",
            "[2/4] Network-level metrics (baseline) ...\n",
            "  - baseline computed in 9.0s\n",
            "[2/4] Bootstrap CIs ...\n",
            "  - bootstrap CI (metric_avg_clustering): n_boot=1000\n"
          ]
        }
      ]
    }
  ]
}